{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"115_hw_05.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5bdd500a8e984cad8313e60c8f2966c8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8b05b7469e6142a9bb4d86c9d466fa33","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4fd7e859e77e475785530187049bd15b","IPY_MODEL_d6eed99c37dd4ef881af200f7cd1f5ae"]}},"8b05b7469e6142a9bb4d86c9d466fa33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4fd7e859e77e475785530187049bd15b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8f63a092ecf343e8abc9fa91af75ceef","_dom_classes":[],"description":"Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.2.2.json: ","_model_name":"FloatProgressModel","bar_style":"success","max":23856,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":23856,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1efff42a44724a91a596b251d954e856"}},"d6eed99c37dd4ef881af200f7cd1f5ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3a52e7036e08428b9cff66a3d18b285f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 139k/? [00:00&lt;00:00, 2.60MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2c274ef21c4c443b820d1af0765d99e3"}},"8f63a092ecf343e8abc9fa91af75ceef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1efff42a44724a91a596b251d954e856":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3a52e7036e08428b9cff66a3d18b285f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2c274ef21c4c443b820d1af0765d99e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"94a8435635dd4158b066e9cb463e0950":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_47b20d4afa3b4118a3d217bc2a0a9b40","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0f1e9d44dc914391b56e6a61f4378c2a","IPY_MODEL_06f01950e8504beb84d9da11a956bb5e"]}},"47b20d4afa3b4118a3d217bc2a0a9b40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0f1e9d44dc914391b56e6a61f4378c2a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_321a7902e6c8480b875cc368128034b4","_dom_classes":[],"description":"Downloading http://nlp.stanford.edu/software/stanza/1.2.2/ru/default.zip: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":574066147,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":574066147,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_475f5dcb4e234f209a2d6be444022590"}},"06f01950e8504beb84d9da11a956bb5e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a25e94f82f50473d8bb6c43a626fa16f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 574M/574M [01:54&lt;00:00, 5.03MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f380c5ef0529432c9d5f67d1c7d106f6"}},"321a7902e6c8480b875cc368128034b4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"475f5dcb4e234f209a2d6be444022590":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a25e94f82f50473d8bb6c43a626fa16f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f380c5ef0529432c9d5f67d1c7d106f6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lT-d5VEPYEfT"},"source":["# Курс Введение в обработку естественного языка / Практическое задание урока 5. Part-of-Speech разметка, NER, извлечение отношений"]},{"cell_type":"markdown","metadata":{"id":"IkpcHsV8RWHA"},"source":["## Задание 1"]},{"cell_type":"markdown","metadata":{"id":"aAQBOJRARev7"},"source":["**Написать теггер на данных с руским языком**\n","1. проверить UnigramTagger, BigramTagger, TrigramTagger и их комбмнации  \n","2. написать свой теггер как на занятии, но улучшить попробовать разные векторайзеры, добавить знание не только букв и слов но и совместно объединить эти признаки  \n","3. вместо векторайзеров взять эмбединги попробовать (word2vec и fasttext по желанию дополнительно можно взять tf.keras.layers.Embedding)  \n","4. взять не только эмбединги каждого слова, но и взять соседей, т.е. информацию о соседях количество соседей выбрать самим (узнать наилучшее количество соседей)    \n","5. сравнить все реализованные методы сделать выводы"]},{"cell_type":"markdown","metadata":{"id":"_16J0ER8WOJx"},"source":["### загрузка данных"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yPRx8Cu_RDY1","executionInfo":{"status":"ok","timestamp":1625681061966,"user_tz":-180,"elapsed":4489,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"8da3b709-60d6-489a-f955-9b983f7d59e0"},"source":["!pip install pyconll"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pyconll\n","  Downloading https://files.pythonhosted.org/packages/0a/4c/edf12b4b211f8a0f7f85a52ed4b50cd453ac96e9b751427e0296eb7ae42a/pyconll-3.1.0-py3-none-any.whl\n","Installing collected packages: pyconll\n","Successfully installed pyconll-3.1.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9wgL-33mWUyZ"},"source":["import pyconll"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXxwW9NzW570"},"source":["!mkdir datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tpwgA3svWiRw","executionInfo":{"status":"ok","timestamp":1625681064286,"user_tz":-180,"elapsed":2328,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"21e1f5ab-7fc4-4a9e-9039-8f5c028f6638"},"source":["!wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n","!wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-07-07 18:04:19--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 81039282 (77M) [text/plain]\n","Saving to: ‘./datasets/ru_syntagrus-ud-train.conllu’\n","\n","./datasets/ru_synta 100%[===================>]  77.28M   106MB/s    in 0.7s    \n","\n","2021-07-07 18:04:21 (106 MB/s) - ‘./datasets/ru_syntagrus-ud-train.conllu’ saved [81039282/81039282]\n","\n","--2021-07-07 18:04:21--  https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10902738 (10M) [text/plain]\n","Saving to: ‘./datasets/ru_syntagrus-ud-dev.conllu’\n","\n","./datasets/ru_synta 100%[===================>]  10.40M  --.-KB/s    in 0.1s    \n","\n","2021-07-07 18:04:21 (96.9 MB/s) - ‘./datasets/ru_syntagrus-ud-dev.conllu’ saved [10902738/10902738]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Oymo30RBWjjl"},"source":["full_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\n","full_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XBzFe82cXGNK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625681088705,"user_tz":-180,"elapsed":13,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"4a8513ac-84f6-40ee-e562-b4cf9796ccbd"},"source":["for sent in full_train[:2]:\n","    for token in sent:\n","        print(token.form, token.upos)\n","    print()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Анкета NOUN\n",". PUNCT\n","\n","Начальник NOUN\n","областного ADJ\n","управления NOUN\n","связи NOUN\n","Семен PROPN\n","Еремеевич PROPN\n","был AUX\n","человек NOUN\n","простой ADJ\n",", PUNCT\n","приходил VERB\n","на ADP\n","работу NOUN\n","всегда ADV\n","вовремя ADV\n",", PUNCT\n","здоровался VERB\n","с ADP\n","секретаршей NOUN\n","за ADP\n","руку NOUN\n","и CCONJ\n","иногда ADV\n","даже PART\n","писал VERB\n","в ADP\n","стенгазету NOUN\n","заметки NOUN\n","под ADP\n","псевдонимом NOUN\n","\" PUNCT\n","Муха NOUN\n","\" PUNCT\n",". PUNCT\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uHzwiXUYftI_"},"source":["import nltk\n","\n","from nltk.corpus import brown\n","from nltk.tag import DefaultTagger\n","from nltk.tag import UnigramTagger\n","from nltk.tag import BigramTagger, TrigramTagger\n","from nltk.tag import RegexpTagger"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tSi8niRiaJ3w"},"source":["def conver_to_test_data(full):\n","    res = []\n","    for sent in full:\n","        sub_res = []\n","        for token in sent:\n","            sub_res.append((token.form, token.upos))\n","        res.append(sub_res)\n","    return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mWlDy5Fxd3Gt"},"source":["def conver_to_test_sent(full):\n","    res = []\n","    for sent in full:\n","        sub_res = []\n","        for token in sent:\n","            sub_res.append(token.form)\n","        res.append(sub_res)\n","    return res"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YSmy1yrNeBF4"},"source":["train_data = conver_to_test_data(full_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vs39V07aemYu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625681091002,"user_tz":-180,"elapsed":23,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"4a63a364-b2f3-4a62-8e25-3dae1f886f5e"},"source":["test_data = conver_to_test_data(full_test)\n","print(test_data[0:10])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[('Алгоритм', 'NOUN'), (',', 'PUNCT'), ('от', 'ADP'), ('имени', 'NOUN'), ('учёного', 'NOUN'), ('аль', 'PART'), ('-', 'PUNCT'), ('Хорезми', 'PROPN'), (',', 'PUNCT'), ('-', 'PUNCT'), ('точный', 'ADJ'), ('набор', 'NOUN'), ('инструкций', 'NOUN'), (',', 'PUNCT'), ('описывающих', 'VERB'), ('порядок', 'NOUN'), ('действий', 'NOUN'), ('исполнителя', 'NOUN'), ('для', 'ADP'), ('достижения', 'NOUN'), ('результата', 'NOUN'), ('решения', 'NOUN'), ('задачи', 'NOUN'), ('за', 'ADP'), ('конечное', 'ADJ'), ('время', 'NOUN'), ('.', 'PUNCT')], [('В', 'ADP'), ('старой', 'ADJ'), ('трактовке', 'NOUN'), ('вместо', 'ADP'), ('слова', 'NOUN'), ('\"', 'PUNCT'), ('порядок', 'NOUN'), ('\"', 'PUNCT'), ('использовалось', 'VERB'), ('слово', 'NOUN'), ('\"', 'PUNCT'), ('последовательность', 'NOUN'), ('\"', 'PUNCT'), (',', 'PUNCT'), ('но', 'CCONJ'), ('по', 'ADP'), ('мере', 'NOUN'), ('развития', 'NOUN'), ('параллельности', 'NOUN'), ('в', 'ADP'), ('работе', 'NOUN'), ('компьютеров', 'NOUN'), ('слово', 'NOUN'), ('\"', 'PUNCT'), ('последовательность', 'NOUN'), ('\"', 'PUNCT'), ('стали', 'VERB'), ('заменять', 'VERB'), ('более', 'ADV'), ('общим', 'ADJ'), ('словом', 'NOUN'), ('\"', 'PUNCT'), ('порядок', 'NOUN'), ('\"', 'PUNCT'), ('.', 'PUNCT')], [('Это', 'PRON'), ('связано', 'VERB'), ('с', 'ADP'), ('тем', 'PRON'), (',', 'PUNCT'), ('что', 'SCONJ'), ('работа', 'NOUN'), ('каких-то', 'DET'), ('инструкций', 'NOUN'), ('алгоритма', 'NOUN'), ('может', 'VERB'), ('быть', 'AUX'), ('зависима', 'ADJ'), ('от', 'ADP'), ('других', 'ADJ'), ('инструкций', 'NOUN'), ('или', 'CCONJ'), ('результатов', 'NOUN'), ('их', 'DET'), ('работы', 'NOUN'), ('.', 'PUNCT')], [('Таким', 'DET'), ('образом', 'NOUN'), (',', 'PUNCT'), ('некоторые', 'DET'), ('инструкции', 'NOUN'), ('должны', 'ADJ'), ('выполняться', 'VERB'), ('строго', 'ADV'), ('после', 'ADP'), ('завершения', 'NOUN'), ('работы', 'NOUN'), ('инструкций', 'NOUN'), (',', 'PUNCT'), ('от', 'ADP'), ('которых', 'PRON'), ('они', 'PRON'), ('зависят', 'VERB'), ('.', 'PUNCT')], [('Независимые', 'ADJ'), ('инструкции', 'NOUN'), ('или', 'CCONJ'), ('инструкции', 'NOUN'), (',', 'PUNCT'), ('ставшие', 'VERB'), ('независимыми', 'ADJ'), ('из-за', 'ADP'), ('завершения', 'NOUN'), ('работы', 'NOUN'), ('инструкций', 'NOUN'), (',', 'PUNCT'), ('от', 'ADP'), ('которых', 'PRON'), ('они', 'PRON'), ('зависят', 'VERB'), (',', 'PUNCT'), ('могут', 'VERB'), ('выполняться', 'VERB'), ('в', 'ADP'), ('произвольном', 'ADJ'), ('порядке', 'NOUN'), (',', 'PUNCT'), ('параллельно', 'ADV'), ('или', 'CCONJ'), ('одновременно', 'ADV'), (',', 'PUNCT'), ('если', 'SCONJ'), ('это', 'PRON'), ('позволяют', 'VERB'), ('используемые', 'VERB'), ('процессор', 'NOUN'), ('и', 'CCONJ'), ('операционная', 'ADJ'), ('система', 'NOUN'), ('.', 'PUNCT')], [('Ранее', 'ADV'), ('часто', 'ADV'), ('писали', 'VERB'), ('\"', 'PUNCT'), ('алгорифм', 'NOUN'), ('\"', 'PUNCT'), (',', 'PUNCT'), ('сейчас', 'ADV'), ('такое', 'DET'), ('написание', 'NOUN'), ('используется', 'VERB'), ('редко', 'ADV'), (',', 'PUNCT'), ('но', 'CCONJ'), (',', 'PUNCT'), ('тем', 'PRON'), ('не', 'PART'), ('менее', 'ADV'), (',', 'PUNCT'), ('имеет', 'VERB'), ('место', 'NOUN'), ('(', 'PUNCT'), ('например', 'ADV'), (',', 'PUNCT'), ('Нормальный', 'ADJ'), ('алгорифм', 'NOUN'), ('Маркова', 'PROPN'), (')', 'PUNCT'), ('.', 'PUNCT')], [('Часто', 'ADV'), ('в', 'ADP'), ('качестве', 'NOUN'), ('исполнителя', 'NOUN'), ('выступает', 'VERB'), ('некоторый', 'DET'), ('механизм', 'NOUN'), ('(', 'PUNCT'), ('компьютер', 'NOUN'), (',', 'PUNCT'), ('токарный', 'ADJ'), ('станок', 'NOUN'), (',', 'PUNCT'), ('швейная', 'ADJ'), ('машина', 'NOUN'), (')', 'PUNCT'), (',', 'PUNCT'), ('но', 'CCONJ'), ('понятие', 'NOUN'), ('алгоритма', 'NOUN'), ('необязательно', 'ADV'), ('относится', 'VERB'), ('к', 'ADP'), ('компьютерным', 'ADJ'), ('программам', 'NOUN'), (',', 'PUNCT'), ('так', 'ADV'), (',', 'PUNCT'), ('например', 'ADV'), (',', 'PUNCT'), ('чётко', 'ADV'), ('описанный', 'VERB'), ('рецепт', 'NOUN'), ('приготовления', 'NOUN'), ('блюда', 'NOUN'), ('также', 'ADV'), ('является', 'VERB'), ('алгоритмом', 'NOUN'), (',', 'PUNCT'), ('в', 'ADP'), ('таком', 'DET'), ('случае', 'NOUN'), ('исполнителем', 'NOUN'), ('является', 'VERB'), ('человек', 'NOUN'), ('.', 'PUNCT')], [('Определения', 'NOUN'), ('алгоритма', 'NOUN'), ('.', 'PUNCT')], [('Единого', 'ADJ'), ('\"', 'PUNCT'), ('истинного', 'ADJ'), ('\"', 'PUNCT'), ('определения', 'NOUN'), ('понятия', 'NOUN'), ('\"', 'PUNCT'), ('алгоритм', 'NOUN'), ('\"', 'PUNCT'), ('нет', 'VERB'), ('.', 'PUNCT')], [('\"', 'PUNCT'), ('Алгоритм', 'NOUN'), ('-', 'PUNCT'), ('это', 'PRON'), ('конечный', 'ADJ'), ('набор', 'NOUN'), ('правил', 'NOUN'), (',', 'PUNCT'), ('который', 'PRON'), ('определяет', 'VERB'), ('последовательность', 'NOUN'), ('операций', 'NOUN'), ('для', 'ADP'), ('решения', 'NOUN'), ('конкретного', 'ADJ'), ('множества', 'NOUN'), ('задач', 'NOUN'), ('и', 'CCONJ'), ('обладает', 'VERB'), ('пятью', 'NUM'), ('важными', 'ADJ'), ('чертами', 'NOUN'), (':', 'PUNCT'), ('конечность', 'NOUN'), (',', 'PUNCT'), ('определённость', 'NOUN'), (',', 'PUNCT'), ('ввод', 'NOUN'), (',', 'PUNCT'), ('вывод', 'NOUN'), (',', 'PUNCT'), ('эффективность', 'NOUN'), ('\"', 'PUNCT'), ('.', 'PUNCT'), ('(', 'PUNCT'), ('Д.', 'PROPN'), ('Э.', 'PROPN'), ('Кнут', 'PROPN'), (')', 'PUNCT'), ('.', 'PUNCT')]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5LUEMNtLez-4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625681091003,"user_tz":-180,"elapsed":19,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"9b6589f4-cbb4-484d-bffe-c8304452881a"},"source":["test_sent = conver_to_test_sent(full_test)[0]\n","print(test_sent)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Алгоритм', ',', 'от', 'имени', 'учёного', 'аль', '-', 'Хорезми', ',', '-', 'точный', 'набор', 'инструкций', ',', 'описывающих', 'порядок', 'действий', 'исполнителя', 'для', 'достижения', 'результата', 'решения', 'задачи', 'за', 'конечное', 'время', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IWSwxjTmZSK3"},"source":["### 1.Проверить UnigramTagger, BigramTagger, TrigramTagger и их комбинации"]},{"cell_type":"code","metadata":{"id":"OshO48XLXQar","colab":{"base_uri":"https://localhost:8080/","height":515},"executionInfo":{"status":"ok","timestamp":1625681096436,"user_tz":-180,"elapsed":5441,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"43faeeb3-41b0-4182-e440-a0a3cfc2a66c"},"source":["unigram_tagger = UnigramTagger(train_data)\n","display(unigram_tagger.tag(test_sent), unigram_tagger.evaluate(test_data))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["[('Алгоритм', None),\n"," (',', 'PUNCT'),\n"," ('от', 'ADP'),\n"," ('имени', 'NOUN'),\n"," ('учёного', 'NOUN'),\n"," ('аль', 'PART'),\n"," ('-', 'PUNCT'),\n"," ('Хорезми', None),\n"," (',', 'PUNCT'),\n"," ('-', 'PUNCT'),\n"," ('точный', 'ADJ'),\n"," ('набор', 'NOUN'),\n"," ('инструкций', 'NOUN'),\n"," (',', 'PUNCT'),\n"," ('описывающих', 'VERB'),\n"," ('порядок', 'NOUN'),\n"," ('действий', 'NOUN'),\n"," ('исполнителя', 'NOUN'),\n"," ('для', 'ADP'),\n"," ('достижения', 'NOUN'),\n"," ('результата', 'NOUN'),\n"," ('решения', 'NOUN'),\n"," ('задачи', 'NOUN'),\n"," ('за', 'ADP'),\n"," ('конечное', None),\n"," ('время', 'NOUN'),\n"," ('.', 'PUNCT')]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["0.8772537323492737"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"dj4tV8ytXTry","colab":{"base_uri":"https://localhost:8080/","height":515},"executionInfo":{"status":"ok","timestamp":1625681102657,"user_tz":-180,"elapsed":6225,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"e614a4af-e784-4031-b136-e99255e2b78e"},"source":["bigram_tagger = BigramTagger(train_data, backoff=unigram_tagger)\n","display(bigram_tagger.tag(test_sent), bigram_tagger.evaluate(test_data))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["[('Алгоритм', None),\n"," (',', 'PUNCT'),\n"," ('от', 'ADP'),\n"," ('имени', 'NOUN'),\n"," ('учёного', 'NOUN'),\n"," ('аль', 'PART'),\n"," ('-', 'PUNCT'),\n"," ('Хорезми', None),\n"," (',', 'PUNCT'),\n"," ('-', 'PUNCT'),\n"," ('точный', 'ADJ'),\n"," ('набор', 'NOUN'),\n"," ('инструкций', 'NOUN'),\n"," (',', 'PUNCT'),\n"," ('описывающих', 'VERB'),\n"," ('порядок', 'NOUN'),\n"," ('действий', 'NOUN'),\n"," ('исполнителя', 'NOUN'),\n"," ('для', 'ADP'),\n"," ('достижения', 'NOUN'),\n"," ('результата', 'NOUN'),\n"," ('решения', 'NOUN'),\n"," ('задачи', 'NOUN'),\n"," ('за', 'ADP'),\n"," ('конечное', None),\n"," ('время', 'NOUN'),\n"," ('.', 'PUNCT')]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["0.8829828463586425"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"spqg0o-LgWCA","colab":{"base_uri":"https://localhost:8080/","height":515},"executionInfo":{"status":"ok","timestamp":1625681110957,"user_tz":-180,"elapsed":8307,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"abcd3f65-97eb-42aa-9416-f231874bb240"},"source":["trigram_tagger = TrigramTagger(train_data, backoff=bigram_tagger)\n","display(trigram_tagger.tag(test_sent), trigram_tagger.evaluate(test_data))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["[('Алгоритм', None),\n"," (',', 'PUNCT'),\n"," ('от', 'ADP'),\n"," ('имени', 'NOUN'),\n"," ('учёного', 'NOUN'),\n"," ('аль', 'PART'),\n"," ('-', 'PUNCT'),\n"," ('Хорезми', None),\n"," (',', 'PUNCT'),\n"," ('-', 'PUNCT'),\n"," ('точный', 'ADJ'),\n"," ('набор', 'NOUN'),\n"," ('инструкций', 'NOUN'),\n"," (',', 'PUNCT'),\n"," ('описывающих', 'VERB'),\n"," ('порядок', 'NOUN'),\n"," ('действий', 'NOUN'),\n"," ('исполнителя', 'NOUN'),\n"," ('для', 'ADP'),\n"," ('достижения', 'NOUN'),\n"," ('результата', 'NOUN'),\n"," ('решения', 'NOUN'),\n"," ('задачи', 'NOUN'),\n"," ('за', 'ADP'),\n"," ('конечное', None),\n"," ('время', 'NOUN'),\n"," ('.', 'PUNCT')]"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/plain":["0.882081353418933"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"hs3qK-VNgqrH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625681131605,"user_tz":-180,"elapsed":20673,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"70c8e390-3765-4108-f575-020e9547962f"},"source":["# Комбинация тэггеров\n","def backoff_tagger(train_sents, tagger_classes, backoff=None):\n","    for cls in tagger_classes:\n","        backoff = cls(train_sents, backoff=backoff)\n","    return backoff\n","\n","\n","backoff = DefaultTagger('NN') \n","tag = backoff_tagger(train_data,  \n","                     [UnigramTagger, BigramTagger, TrigramTagger],  \n","                     backoff = backoff) \n","  \n","tag.evaluate(test_data)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8814747413473528"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"BK-MGJLYhvOx"},"source":["### 2.Написать свой теггер как на занятии, но улучшить попробовать разные векторайзеры, добавить знание не только букв и слов но и совместно объединить эти признаки"]},{"cell_type":"code","metadata":{"id":"qs01YoOIkR4n"},"source":["from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n","\n","from sklearn.pipeline import make_pipeline\n","from sklearn.preprocessing import StandardScaler\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import LabelEncoder"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2CP2Eu8Oii9U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625681133714,"user_tz":-180,"elapsed":2113,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"05534af1-0708-4fef-d40d-b712a60d90aa"},"source":["fdata_train = []\n","for sent in full_train[:]:\n","    fdata_train.append([(token.form.lower(), token.upos) for token in sent])\n","    \n","fdata_test = []\n","for sent in full_test[:]:\n","    fdata_test.append([(token.form.lower(), token.upos) for token in sent])\n","    \n","fdata_sent_train = []\n","for sent in full_train[:]:\n","    fdata_sent_train.append([token.form.lower() for token in sent])\n","    \n","fdata_sent_test = []\n","for sent in full_test[:]:\n","    fdata_sent_test.append([token.form.lower() for token in sent])\n","    \n","    \n","MAX_SENT_LEN = max(len(sent) for sent in full_train)\n","MAX_ORIG_TOKEN_LEN = max(len(token.form) for sent in full_train for token in sent)\n","print('Наибольшая длина предложения', MAX_SENT_LEN)\n","print('Наибольшая длина токена', MAX_ORIG_TOKEN_LEN)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Наибольшая длина предложения 205\n","Наибольшая длина токена 47\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RUoLh3i9i7of","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625681134671,"user_tz":-180,"elapsed":963,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"13de87c3-3584-4814-9205-e7800b09c8a2"},"source":["train_tok = []\n","train_label = []\n","for sent in fdata_train[:]:\n","    for tok in sent:\n","        train_tok.append(tok[0])\n","        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n","        \n","test_tok = []\n","test_label = []\n","for sent in fdata_test[:]:\n","    for tok in sent:\n","        test_tok.append(tok[0])\n","        test_label.append('NO_TAG' if tok[1] is None else tok[1])\n","\n","le = LabelEncoder()\n","train_enc_labels = le.fit_transform(train_label) \n","test_enc_labels = le.transform(test_label)\n","le.classes_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n","       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n","       'VERB', 'X'], dtype='<U6')"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"13k3JKnMjGpr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625682168127,"user_tz":-180,"elapsed":1033459,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"517cc0e1-c734-4b52-c0fc-81fea6592f71"},"source":["# берем разные векторайзеры, анализируем по буквам\n","for vectorizer in [CountVectorizer, HashingVectorizer, TfidfVectorizer]:\n","\n","    scaler = StandardScaler(with_mean=False)\n","    coder = vectorizer(ngram_range=(1, 5), analyzer='char')\n","    \n","\n","    X_train = coder.fit_transform(train_tok)\n","    X_test = coder.transform(test_tok)\n","    \n","    X_train = scaler.fit_transform(X_train)\n","    X_test = scaler.fit_transform(X_test)    \n","    \n","    \n","    print(X_train.shape)\n","    lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n","    lr.fit(X_train, train_enc_labels)\n","\n","    pred = lr.predict(X_test)\n","\n","    print(vectorizer, accuracy_score(test_enc_labels, pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(871526, 149809)\n","<class 'sklearn.feature_extraction.text.CountVectorizer'> 0.943644053516665\n","(871526, 1048576)\n","<class 'sklearn.feature_extraction.text.HashingVectorizer'> 0.9471826239342163\n","(871526, 149809)\n","<class 'sklearn.feature_extraction.text.TfidfVectorizer'> 0.9487749806221144\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oCUF1KMgjLy9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625682918322,"user_tz":-180,"elapsed":750208,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"15932d5a-fa46-4872-8cbd-738283a66163"},"source":["# берем разные векторайзеры, анализируем по словам\n","for vectorizer in [CountVectorizer, HashingVectorizer, TfidfVectorizer]:\n","\n","    scaler = StandardScaler(with_mean=False)\n","    coder = vectorizer(ngram_range=(1, 5), analyzer='word')\n","    \n","\n","    X_train = coder.fit_transform(train_tok)\n","    X_test = coder.transform(test_tok)\n","    \n","    X_train = scaler.fit_transform(X_train)\n","    X_test = scaler.fit_transform(X_test)    \n","    \n","    \n","    print(X_train.shape)\n","    lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=7)\n","    lr.fit(X_train, train_enc_labels)\n","\n","    pred = lr.predict(X_test)\n","\n","    print(vectorizer, accuracy_score(test_enc_labels, pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(871526, 99485)\n","<class 'sklearn.feature_extraction.text.CountVectorizer'> 0.7531847133757962\n","(871526, 1048576)\n","<class 'sklearn.feature_extraction.text.HashingVectorizer'> 0.7722255922892866\n","(871526, 99485)\n","<class 'sklearn.feature_extraction.text.TfidfVectorizer'> 0.7532605398847437\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yl8x7Vk3jdyx","executionInfo":{"status":"ok","timestamp":1625683544165,"user_tz":-180,"elapsed":613009,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"f9b748eb-c59b-47ef-c86f-8940a0151b58"},"source":["# В случае совмещения двух признаков (буквы, слова) код успешно отработал только при первом запуске\n","# При последующих запусках выдавал ошибку. \n","# Для ликвидации ошибки делалось уменьшение n_jobs. \n","# После запуска в другой день и установки  n_jobs=4 ошибка не выдалась.\n","import numpy as np\n","from scipy.sparse import hstack\n","\n","# берем векторайзер TfidfVectorizer по буквам и HashingVectorizer по словам\n","scaler = StandardScaler(with_mean=False)\n","coder_1 = TfidfVectorizer(ngram_range=(1, 5), analyzer='char')\n","coder_2 = HashingVectorizer(ngram_range=(1, 5), analyzer='word')\n","\n","X_train_1 = coder_1.fit_transform(train_tok)\n","X_test_1 = coder_1.transform(test_tok)\n","\n","X_train_2 = coder_2.fit_transform(train_tok)\n","X_test_2 = coder_2.transform(test_tok)\n","\n","\n","X_train = hstack((X_train_1,X_train_2))\n","X_test = hstack((X_test_1,X_test_2))\n","\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.fit_transform(X_test)    \n","\n","\n","print(X_train.shape)\n","lr = LogisticRegression(random_state=0, max_iter = 100, n_jobs=4)\n","lr.fit(X_train, train_enc_labels)\n","\n","pred = lr.predict(X_test)\n","\n","print('TfidfVectorizer_char + HashingVectorizer_word :', accuracy_score(test_enc_labels, pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(871526, 1198385)\n","TfidfVectorizer_char + HashingVectorizer_word : 0.9441327132409935\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FcC3NCmdvURC"},"source":["### 3.Вместо векторайзеров взять эмбединги попробовать (word2vec и fasttext по желанию дополнительно можно взять tf.keras.layers.Embedding)"]},{"cell_type":"code","metadata":{"id":"B0gILQZqyZCh"},"source":["import gensim"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"um4dnCGL3ryx"},"source":["#### word2vec"]},{"cell_type":"code","metadata":{"id":"0-FcVYByvc4m"},"source":["train_word2vec = fdata_sent_train\n","train_word2vec.extend(fdata_sent_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E9niuMXfvdDM","executionInfo":{"status":"ok","timestamp":1625683968607,"user_tz":-180,"elapsed":82086,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"ba52dbf9-1566-46ae-d632-c88c8811dc7b"},"source":["w2v_model = gensim.models.Word2Vec(min_count=1, negative=20, sg=1, window=2)\n","w2v_model.build_vocab(train_word2vec)\n","w2v_model.train(train_word2vec, epochs=5, total_examples=w2v_model.corpus_count)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3850208, 4951090)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"XReGHwQXvl07"},"source":["X_train_w2v = [w2v_model.wv[word] for word in train_tok]\n","X_test_w2v = [w2v_model.wv[word] for word in test_tok]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GNEObdnBvl5j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625684041482,"user_tz":-180,"elapsed":64794,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"e927338e-2f6a-4a66-b1af-3b8b011abc99"},"source":["lr = LogisticRegression(random_state=0, max_iter=35) # max_iter изменено с 25 на 35\n","lr.fit(X_train_w2v, train_enc_labels)\n","pred = lr.predict(X_test_w2v)\n","accuracy_score(test_enc_labels, pred)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["0.7469837899774205"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"y6J-csTOv93p"},"source":["#### fasttext"]},{"cell_type":"code","metadata":{"id":"YvIDDxfuvmFO"},"source":["ft_model = gensim.models.FastText(train_word2vec, min_count=1, negative=20, sg=1, window=5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J_KlvuR0jlKI"},"source":["X_train_ft = [ft_model.wv[word] for word in train_tok]\n","X_test_ft = [ft_model.wv[word] for word in test_tok]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lin_tB14wFRL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625684852135,"user_tz":-180,"elapsed":59317,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"deedead4-2df0-42c1-ebdd-ddf07bd39b3c"},"source":["lr = LogisticRegression(random_state=0, max_iter=35) # max_iter изменено с 25 на 35\n","lr.fit(X_train_ft, train_enc_labels)\n","pred = lr.predict(X_test_ft)\n","accuracy_score(test_enc_labels, pred)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["0.8727631179860479"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"oNa-Hmll0I79"},"source":["#### 4.взять не только эмбединги каждого слова, но и взять соседей, т.е. информацию о соседях количество соседей выбрать самим (узнать наилучшее количество соседей)"]},{"cell_type":"code","metadata":{"id":"7UjTFqAd0OFq"},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TAvF7GMW2TVW"},"source":["lr = LogisticRegression(random_state=0, max_iter=25)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4f8Xts-j1ACH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625686415188,"user_tz":-180,"elapsed":1531980,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"7b29d4a1-49e2-4624-d62f-c18f5a99c6ec"},"source":["windows = [1,2,3,4,5,6,7,8,9]\n","for i in windows:\n","    w2v_model = gensim.models.Word2Vec(min_count=1, negative=20, sg=1, window=i)\n","    w2v_model.build_vocab(train_word2vec)\n","    w2v_model.train(train_word2vec, epochs=5, total_examples=w2v_model.corpus_count)\n","    X_train_w2v = [w2v_model.wv[word] for word in train_tok]\n","    X_test_w2v = [w2v_model.wv[word] for word in test_tok]\n","    lr.fit(X_train_w2v, train_enc_labels)\n","    pred = lr.predict(X_test_w2v)\n","    print('word2vec, window=', i, accuracy_score(test_enc_labels, pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["word2vec, window= 1 0.7483992181444411\n","word2vec, window= 2 0.7394432649209719\n","word2vec, window= 3 0.7370505173052944\n","word2vec, window= 4 0.7279766117345735\n","word2vec, window= 5 0.7279681865669126\n","word2vec, window= 6 0.7235449735449735\n","word2vec, window= 7 0.7155242139318573\n","word2vec, window= 8 0.7166868870690527\n","word2vec, window= 9 0.7158696458059516\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e46O5GO90OPy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625419228656,"user_tz":-180,"elapsed":1375243,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"3ded17e7-6a93-4e45-a656-6276b12cba4d"},"source":["windows = [1,2,3,4,5,6,7,8,9]\n","for i in windows:\n","    ft_model = gensim.models.FastText(train_word2vec, min_count=1, negative=20, sg=1, window=i)\n","    X_train_ft = [ft_model.wv[word] for word in train_tok]\n","    X_test_ft = [ft_model.wv[word] for word in test_tok]\n","    lr.fit(X_train_ft, train_enc_labels)\n","    pred = lr.predict(X_test_ft)\n","    print('fasttext, window=', i, accuracy_score(test_enc_labels, pred))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["fasttext, window= 1 0.8582886799447309\n","fasttext, window= 2 0.864000943618778\n","fasttext, window= 3 0.8595440299261955\n","fasttext, window= 4 0.8590553702018671\n","fasttext, window= 5 0.8587015131601119\n","fasttext, window= 6 0.851380042462845\n","fasttext, window= 7 0.8503100461699188\n","fasttext, window= 8 0.8453476224176861\n","fasttext, window= 9 0.8446651838371584\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8jqUCMqogZr6"},"source":["#### 5.сравнить все реализованные методы сделать выводы"]},{"cell_type":"markdown","metadata":{"id":"bNHqFMrhgvbi"},"source":["* 0.88147 - скор по комбинации из UnigramTagger BigramTagger TrigramTagger при методе nltk.tag\n","* 0.94877 - скор LogisticRegression в сочетании с TfidfVectorizer по буквам при методе Vectorizer\n","* 0.86400 - скор LogisticRegression в сочетании с  FastText по window= 2 при методе эмбедингов \n","\n","**Вывод:**\n","* для рассматриваемого корпуса наилучшим является исследование по буквам, а не по словам \n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Oo3zvsJ20Ob1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cINqgGpKXURp"},"source":["## Задание 2"]},{"cell_type":"markdown","metadata":{"id":"VCM0drjKXYet"},"source":["много дополнительных датасетов на русском языке\n","\n","https://natasha.github.io/corus/  \n","https://github.com/natasha/corus"]},{"cell_type":"markdown","metadata":{"id":"sUOg4C8sZNpw"},"source":["мы будем использовать данные http://www.labinform.ru/pub/named_entities/"]},{"cell_type":"markdown","metadata":{"id":"qzi6ApNLZg6X"},"source":["**Проверить насколько хорошо работает NER**\n","\n","1. взять нер из nltk\n","2. проверить deeppavlov\n","3. написать свой нер попробовать разные подходы (с доп информацией без) так же с учётом соседей и без них\n","4. сделать выводы по вашим экспериментам какой из подходов успешнее справляется"]},{"cell_type":"markdown","metadata":{"id":"aP1LgaNUtaOz"},"source":["при обучении своего нера не забудьте разделить выборку"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qg6tcss2Zhp9","executionInfo":{"status":"ok","timestamp":1626548157026,"user_tz":-180,"elapsed":3787,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"a0ea5c82-45e1-4292-ff93-5e4e95283a22"},"source":["!pip install corus"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting corus\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e4/a3/e680679c669b0118271ac7246549c75a7088ab0e6696a3561408a3f9b50d/corus-0.9.0-py3-none-any.whl (83kB)\n","\r\u001b[K     |████                            | 10kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 20kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 30kB 6.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 40kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 51kB 3.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 61kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 71kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 81kB 4.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 92kB 3.8MB/s \n","\u001b[?25hInstalling collected packages: corus\n","Successfully installed corus-0.9.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hrc5ocDkaS1e","executionInfo":{"status":"ok","timestamp":1626548160058,"user_tz":-180,"elapsed":417,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}}},"source":["import corus"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vPVv6lhT3ftA","executionInfo":{"status":"ok","timestamp":1626548173992,"user_tz":-180,"elapsed":11351,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"bda3599b-2162-4ff3-b9a0-833725eac9ec"},"source":["!wget http://www.labinform.ru/pub/named_entities/collection5.zip"],"execution_count":4,"outputs":[{"output_type":"stream","text":["--2021-07-17 18:56:02--  http://www.labinform.ru/pub/named_entities/collection5.zip\n","Resolving www.labinform.ru (www.labinform.ru)... 80.240.100.4\n","Connecting to www.labinform.ru (www.labinform.ru)|80.240.100.4|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1899530 (1.8M) [application/zip]\n","Saving to: ‘collection5.zip’\n","\n","collection5.zip     100%[===================>]   1.81M   226KB/s    in 9.5s    \n","\n","2021-07-17 18:56:13 (196 KB/s) - ‘collection5.zip’ saved [1899530/1899530]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XtLK4z9CH2Ph","executionInfo":{"status":"ok","timestamp":1626548178821,"user_tz":-180,"elapsed":1003,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"d427131c-5976-4534-de62-221df5fa73c1"},"source":["!unzip collection5.zip"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Archive:  collection5.zip\n","   creating: Collection5/\n","  inflating: Collection5/001.ann     \n","  inflating: Collection5/001.txt     \n","  inflating: Collection5/002.ann     \n","  inflating: Collection5/002.txt     \n","  inflating: Collection5/003.ann     \n","  inflating: Collection5/003.txt     \n","  inflating: Collection5/004.ann     \n","  inflating: Collection5/004.txt     \n","  inflating: Collection5/005.ann     \n","  inflating: Collection5/005.txt     \n","  inflating: Collection5/006.ann     \n","  inflating: Collection5/006.txt     \n","  inflating: Collection5/007.ann     \n","  inflating: Collection5/007.txt     \n","  inflating: Collection5/008.ann     \n","  inflating: Collection5/008.txt     \n","  inflating: Collection5/009.ann     \n","  inflating: Collection5/009.txt     \n","  inflating: Collection5/010.ann     \n","  inflating: Collection5/010.txt     \n","  inflating: Collection5/011.ann     \n","  inflating: Collection5/011.txt     \n","  inflating: Collection5/012.ann     \n","  inflating: Collection5/012.txt     \n","  inflating: Collection5/013.ann     \n","  inflating: Collection5/013.txt     \n","  inflating: Collection5/014.ann     \n","  inflating: Collection5/014.txt     \n","  inflating: Collection5/015 (!).ann  \n","  inflating: Collection5/015 (!).txt  \n","  inflating: Collection5/016.ann     \n","  inflating: Collection5/016.txt     \n","  inflating: Collection5/017.ann     \n","  inflating: Collection5/017.txt     \n","  inflating: Collection5/018.ann     \n","  inflating: Collection5/018.txt     \n","  inflating: Collection5/019.ann     \n","  inflating: Collection5/019.txt     \n","  inflating: Collection5/020.ann     \n","  inflating: Collection5/020.txt     \n","  inflating: Collection5/021.ann     \n","  inflating: Collection5/021.txt     \n","  inflating: Collection5/022.ann     \n","  inflating: Collection5/022.txt     \n","  inflating: Collection5/023.ann     \n","  inflating: Collection5/023.txt     \n","  inflating: Collection5/025.ann     \n","  inflating: Collection5/025.txt     \n","  inflating: Collection5/026.ann     \n","  inflating: Collection5/026.txt     \n","  inflating: Collection5/027.ann     \n","  inflating: Collection5/027.txt     \n","  inflating: Collection5/028.ann     \n","  inflating: Collection5/028.txt     \n","  inflating: Collection5/029.ann     \n","  inflating: Collection5/029.txt     \n","  inflating: Collection5/030.ann     \n","  inflating: Collection5/030.txt     \n","  inflating: Collection5/031.ann     \n","  inflating: Collection5/031.txt     \n","  inflating: Collection5/032.ann     \n","  inflating: Collection5/032.txt     \n","  inflating: Collection5/033.ann     \n","  inflating: Collection5/033.txt     \n","  inflating: Collection5/034.ann     \n","  inflating: Collection5/034.txt     \n","  inflating: Collection5/035.ann     \n","  inflating: Collection5/035.txt     \n","  inflating: Collection5/036.ann     \n","  inflating: Collection5/036.txt     \n","  inflating: Collection5/037.ann     \n","  inflating: Collection5/037.txt     \n","  inflating: Collection5/038.ann     \n","  inflating: Collection5/038.txt     \n","  inflating: Collection5/039.ann     \n","  inflating: Collection5/039.txt     \n","  inflating: Collection5/03_12_12a.ann  \n","  inflating: Collection5/03_12_12a.txt  \n","  inflating: Collection5/03_12_12b.ann  \n","  inflating: Collection5/03_12_12b.txt  \n","  inflating: Collection5/03_12_12c.ann  \n","  inflating: Collection5/03_12_12c.txt  \n","  inflating: Collection5/03_12_12d.ann  \n","  inflating: Collection5/03_12_12d.txt  \n","  inflating: Collection5/03_12_12g.ann  \n","  inflating: Collection5/03_12_12g.txt  \n","  inflating: Collection5/03_12_12h.ann  \n","  inflating: Collection5/03_12_12h.txt  \n","  inflating: Collection5/040.ann     \n","  inflating: Collection5/040.txt     \n","  inflating: Collection5/041.ann     \n","  inflating: Collection5/041.txt     \n","  inflating: Collection5/042.ann     \n","  inflating: Collection5/042.txt     \n","  inflating: Collection5/043.ann     \n","  inflating: Collection5/043.txt     \n","  inflating: Collection5/044.ann     \n","  inflating: Collection5/044.txt     \n","  inflating: Collection5/045.ann     \n","  inflating: Collection5/045.txt     \n","  inflating: Collection5/046.ann     \n","  inflating: Collection5/046.txt     \n","  inflating: Collection5/047.ann     \n","  inflating: Collection5/047.txt     \n","  inflating: Collection5/048.ann     \n","  inflating: Collection5/048.txt     \n","  inflating: Collection5/049.ann     \n","  inflating: Collection5/049.txt     \n","  inflating: Collection5/04_02_13a_abdulatipov.ann  \n","  inflating: Collection5/04_02_13a_abdulatipov.txt  \n","  inflating: Collection5/04_03_13a_sorokin.ann  \n","  inflating: Collection5/04_03_13a_sorokin.txt  \n","  inflating: Collection5/04_12_12b.ann  \n","  inflating: Collection5/04_12_12b.txt  \n","  inflating: Collection5/04_12_12d.ann  \n","  inflating: Collection5/04_12_12d.txt  \n","  inflating: Collection5/04_12_12f.ann  \n","  inflating: Collection5/04_12_12f.txt  \n","  inflating: Collection5/04_12_12g.ann  \n","  inflating: Collection5/04_12_12g.txt  \n","  inflating: Collection5/04_12_12h_corr.ann  \n","  inflating: Collection5/04_12_12h_corr.txt  \n","  inflating: Collection5/050.ann     \n","  inflating: Collection5/050.txt     \n","  inflating: Collection5/051.ann     \n","  inflating: Collection5/051.txt     \n","  inflating: Collection5/052.ann     \n","  inflating: Collection5/052.txt     \n","  inflating: Collection5/053.ann     \n","  inflating: Collection5/053.txt     \n","  inflating: Collection5/054.ann     \n","  inflating: Collection5/054.txt     \n","  inflating: Collection5/055.ann     \n","  inflating: Collection5/055.txt     \n","  inflating: Collection5/056.ann     \n","  inflating: Collection5/056.txt     \n","  inflating: Collection5/057.ann     \n","  inflating: Collection5/057.txt     \n","  inflating: Collection5/058.ann     \n","  inflating: Collection5/058.txt     \n","  inflating: Collection5/059.ann     \n","  inflating: Collection5/059.txt     \n","  inflating: Collection5/060.ann     \n","  inflating: Collection5/060.txt     \n","  inflating: Collection5/061.ann     \n","  inflating: Collection5/061.txt     \n","  inflating: Collection5/062.ann     \n","  inflating: Collection5/062.txt     \n","  inflating: Collection5/063.ann     \n","  inflating: Collection5/063.txt     \n","  inflating: Collection5/064.ann     \n","  inflating: Collection5/064.txt     \n","  inflating: Collection5/065.ann     \n","  inflating: Collection5/065.txt     \n","  inflating: Collection5/066.ann     \n","  inflating: Collection5/066.txt     \n","  inflating: Collection5/067.ann     \n","  inflating: Collection5/067.txt     \n","  inflating: Collection5/068.ann     \n","  inflating: Collection5/068.txt     \n","  inflating: Collection5/069.ann     \n","  inflating: Collection5/069.txt     \n","  inflating: Collection5/070.ann     \n","  inflating: Collection5/070.txt     \n","  inflating: Collection5/071.ann     \n","  inflating: Collection5/071.txt     \n","  inflating: Collection5/072.ann     \n","  inflating: Collection5/072.txt     \n","  inflating: Collection5/073.ann     \n","  inflating: Collection5/073.txt     \n","  inflating: Collection5/074.ann     \n","  inflating: Collection5/074.txt     \n","  inflating: Collection5/075.ann     \n","  inflating: Collection5/075.txt     \n","  inflating: Collection5/076.ann     \n","  inflating: Collection5/076.txt     \n","  inflating: Collection5/077.ann     \n","  inflating: Collection5/077.txt     \n","  inflating: Collection5/078.ann     \n","  inflating: Collection5/078.txt     \n","  inflating: Collection5/079.ann     \n","  inflating: Collection5/079.txt     \n","  inflating: Collection5/080.ann     \n","  inflating: Collection5/080.txt     \n","  inflating: Collection5/081.ann     \n","  inflating: Collection5/081.txt     \n","  inflating: Collection5/082.ann     \n","  inflating: Collection5/082.txt     \n","  inflating: Collection5/083.ann     \n","  inflating: Collection5/083.txt     \n","  inflating: Collection5/084.ann     \n","  inflating: Collection5/084.txt     \n","  inflating: Collection5/085.ann     \n","  inflating: Collection5/085.txt     \n","  inflating: Collection5/086.ann     \n","  inflating: Collection5/086.txt     \n","  inflating: Collection5/087.ann     \n","  inflating: Collection5/087.txt     \n","  inflating: Collection5/088.ann     \n","  inflating: Collection5/088.txt     \n","  inflating: Collection5/089.ann     \n","  inflating: Collection5/089.txt     \n","  inflating: Collection5/090.ann     \n","  inflating: Collection5/090.txt     \n","  inflating: Collection5/091.ann     \n","  inflating: Collection5/091.txt     \n","  inflating: Collection5/092.ann     \n","  inflating: Collection5/092.txt     \n","  inflating: Collection5/093.ann     \n","  inflating: Collection5/093.txt     \n","  inflating: Collection5/094.ann     \n","  inflating: Collection5/094.txt     \n","  inflating: Collection5/095.ann     \n","  inflating: Collection5/095.txt     \n","  inflating: Collection5/096.ann     \n","  inflating: Collection5/096.txt     \n","  inflating: Collection5/097.ann     \n","  inflating: Collection5/097.txt     \n","  inflating: Collection5/098.ann     \n","  inflating: Collection5/098.txt     \n","  inflating: Collection5/099.ann     \n","  inflating: Collection5/099.txt     \n","  inflating: Collection5/09_01_13.ann  \n","  inflating: Collection5/09_01_13.txt  \n","  inflating: Collection5/09_01_13a.ann  \n","  inflating: Collection5/09_01_13a.txt  \n","  inflating: Collection5/09_01_13c.ann  \n","  inflating: Collection5/09_01_13c.txt  \n","  inflating: Collection5/09_01_13d.ann  \n","  inflating: Collection5/09_01_13d.txt  \n","  inflating: Collection5/09_01_13e.ann  \n","  inflating: Collection5/09_01_13e.txt  \n","  inflating: Collection5/09_01_13h.ann  \n","  inflating: Collection5/09_01_13h.txt  \n","  inflating: Collection5/09_01_13i.ann  \n","  inflating: Collection5/09_01_13i.txt  \n","  inflating: Collection5/100.ann     \n","  inflating: Collection5/100.txt     \n","  inflating: Collection5/1000.ann    \n","  inflating: Collection5/1000.txt    \n","  inflating: Collection5/1001.ann    \n","  inflating: Collection5/1001.txt    \n","  inflating: Collection5/1002.ann    \n","  inflating: Collection5/1002.txt    \n","  inflating: Collection5/1003.ann    \n","  inflating: Collection5/1003.txt    \n","  inflating: Collection5/1004.ann    \n","  inflating: Collection5/1004.txt    \n","  inflating: Collection5/1005.ann    \n","  inflating: Collection5/1005.txt    \n","  inflating: Collection5/1006.ann    \n","  inflating: Collection5/1006.txt    \n","  inflating: Collection5/1007.ann    \n","  inflating: Collection5/1007.txt    \n","  inflating: Collection5/1008.ann    \n","  inflating: Collection5/1008.txt    \n","  inflating: Collection5/1009.ann    \n","  inflating: Collection5/1009.txt    \n","  inflating: Collection5/101.ann     \n","  inflating: Collection5/101.txt     \n","  inflating: Collection5/1010.ann    \n","  inflating: Collection5/1010.txt    \n","  inflating: Collection5/1011.ann    \n","  inflating: Collection5/1011.txt    \n","  inflating: Collection5/1012.ann    \n","  inflating: Collection5/1012.txt    \n","  inflating: Collection5/1013.ann    \n","  inflating: Collection5/1013.txt    \n","  inflating: Collection5/1014.ann    \n","  inflating: Collection5/1014.txt    \n","  inflating: Collection5/1015.ann    \n","  inflating: Collection5/1015.txt    \n","  inflating: Collection5/1016.ann    \n","  inflating: Collection5/1016.txt    \n","  inflating: Collection5/1017.ann    \n","  inflating: Collection5/1017.txt    \n","  inflating: Collection5/1018.ann    \n","  inflating: Collection5/1018.txt    \n","  inflating: Collection5/1019.ann    \n","  inflating: Collection5/1019.txt    \n","  inflating: Collection5/102.ann     \n","  inflating: Collection5/102.txt     \n","  inflating: Collection5/1020.ann    \n","  inflating: Collection5/1020.txt    \n","  inflating: Collection5/1021.ann    \n","  inflating: Collection5/1021.txt    \n","  inflating: Collection5/1022.ann    \n","  inflating: Collection5/1022.txt    \n","  inflating: Collection5/1023.ann    \n","  inflating: Collection5/1023.txt    \n","  inflating: Collection5/1024.ann    \n","  inflating: Collection5/1024.txt    \n","  inflating: Collection5/1025.ann    \n","  inflating: Collection5/1025.txt    \n","  inflating: Collection5/1026.ann    \n","  inflating: Collection5/1026.txt    \n","  inflating: Collection5/1027.ann    \n","  inflating: Collection5/1027.txt    \n","  inflating: Collection5/1028.ann    \n","  inflating: Collection5/1028.txt    \n","  inflating: Collection5/1029.ann    \n","  inflating: Collection5/1029.txt    \n","  inflating: Collection5/103.ann     \n","  inflating: Collection5/103.txt     \n","  inflating: Collection5/1030.ann    \n","  inflating: Collection5/1030.txt    \n","  inflating: Collection5/1031.ann    \n","  inflating: Collection5/1031.txt    \n","  inflating: Collection5/1032.ann    \n","  inflating: Collection5/1032.txt    \n","  inflating: Collection5/1033.ann    \n","  inflating: Collection5/1033.txt    \n","  inflating: Collection5/1034.ann    \n","  inflating: Collection5/1034.txt    \n","  inflating: Collection5/1035.ann    \n","  inflating: Collection5/1035.txt    \n","  inflating: Collection5/1036.ann    \n","  inflating: Collection5/1036.txt    \n","  inflating: Collection5/1037.ann    \n","  inflating: Collection5/1037.txt    \n","  inflating: Collection5/1038.ann    \n","  inflating: Collection5/1038.txt    \n","  inflating: Collection5/1039.ann    \n","  inflating: Collection5/1039.txt    \n","  inflating: Collection5/104.ann     \n","  inflating: Collection5/104.txt     \n","  inflating: Collection5/1040.ann    \n","  inflating: Collection5/1040.txt    \n","  inflating: Collection5/1041.ann    \n","  inflating: Collection5/1041.txt    \n","  inflating: Collection5/1042.ann    \n","  inflating: Collection5/1042.txt    \n","  inflating: Collection5/1043.ann    \n","  inflating: Collection5/1043.txt    \n","  inflating: Collection5/1044.ann    \n","  inflating: Collection5/1044.txt    \n","  inflating: Collection5/1045.ann    \n","  inflating: Collection5/1045.txt    \n","  inflating: Collection5/1046.ann    \n","  inflating: Collection5/1046.txt    \n","  inflating: Collection5/1047.ann    \n","  inflating: Collection5/1047.txt    \n","  inflating: Collection5/1048.ann    \n","  inflating: Collection5/1048.txt    \n","  inflating: Collection5/1049.ann    \n","  inflating: Collection5/1049.txt    \n","  inflating: Collection5/105.ann     \n","  inflating: Collection5/105.txt     \n","  inflating: Collection5/1050.ann    \n","  inflating: Collection5/1050.txt    \n","  inflating: Collection5/106.ann     \n","  inflating: Collection5/106.txt     \n","  inflating: Collection5/107.ann     \n","  inflating: Collection5/107.txt     \n","  inflating: Collection5/108.ann     \n","  inflating: Collection5/108.txt     \n","  inflating: Collection5/109.ann     \n","  inflating: Collection5/109.txt     \n","  inflating: Collection5/10_01_13a.ann  \n","  inflating: Collection5/10_01_13a.txt  \n","  inflating: Collection5/10_01_13d.ann  \n","  inflating: Collection5/10_01_13d.txt  \n","  inflating: Collection5/10_01_13i.ann  \n","  inflating: Collection5/10_01_13i.txt  \n","  inflating: Collection5/110.ann     \n","  inflating: Collection5/110.txt     \n","  inflating: Collection5/1100.ann    \n","  inflating: Collection5/1100.txt    \n","  inflating: Collection5/1101.ann    \n","  inflating: Collection5/1101.txt    \n","  inflating: Collection5/1102.ann    \n","  inflating: Collection5/1102.txt    \n","  inflating: Collection5/1103.ann    \n","  inflating: Collection5/1103.txt    \n","  inflating: Collection5/1104.ann    \n","  inflating: Collection5/1104.txt    \n","  inflating: Collection5/1105.ann    \n","  inflating: Collection5/1105.txt    \n","  inflating: Collection5/1106.ann    \n","  inflating: Collection5/1106.txt    \n","  inflating: Collection5/1107.ann    \n","  inflating: Collection5/1107.txt    \n","  inflating: Collection5/1108.ann    \n","  inflating: Collection5/1108.txt    \n","  inflating: Collection5/1109.ann    \n","  inflating: Collection5/1109.txt    \n","  inflating: Collection5/111.ann     \n","  inflating: Collection5/111.txt     \n","  inflating: Collection5/1110.ann    \n","  inflating: Collection5/1110.txt    \n","  inflating: Collection5/1111.ann    \n","  inflating: Collection5/1111.txt    \n","  inflating: Collection5/1112.ann    \n","  inflating: Collection5/1112.txt    \n","  inflating: Collection5/1113.ann    \n","  inflating: Collection5/1113.txt    \n","  inflating: Collection5/1114.ann    \n","  inflating: Collection5/1114.txt    \n","  inflating: Collection5/1115.ann    \n","  inflating: Collection5/1115.txt    \n","  inflating: Collection5/1116.ann    \n","  inflating: Collection5/1116.txt    \n","  inflating: Collection5/1117.ann    \n","  inflating: Collection5/1117.txt    \n","  inflating: Collection5/1118.ann    \n","  inflating: Collection5/1118.txt    \n","  inflating: Collection5/1119.ann    \n","  inflating: Collection5/1119.txt    \n","  inflating: Collection5/112.ann     \n","  inflating: Collection5/112.txt     \n","  inflating: Collection5/1120.ann    \n","  inflating: Collection5/1120.txt    \n","  inflating: Collection5/1121.ann    \n","  inflating: Collection5/1121.txt    \n","  inflating: Collection5/1122.ann    \n","  inflating: Collection5/1122.txt    \n","  inflating: Collection5/1123.ann    \n","  inflating: Collection5/1123.txt    \n","  inflating: Collection5/1124.ann    \n","  inflating: Collection5/1124.txt    \n","  inflating: Collection5/1125.ann    \n","  inflating: Collection5/1125.txt    \n","  inflating: Collection5/1126.ann    \n","  inflating: Collection5/1126.txt    \n","  inflating: Collection5/1127.ann    \n","  inflating: Collection5/1127.txt    \n","  inflating: Collection5/1128.ann    \n","  inflating: Collection5/1128.txt    \n","  inflating: Collection5/113.ann     \n","  inflating: Collection5/113.txt     \n","  inflating: Collection5/1130.ann    \n","  inflating: Collection5/1130.txt    \n","  inflating: Collection5/1131.ann    \n","  inflating: Collection5/1131.txt    \n","  inflating: Collection5/1132.ann    \n","  inflating: Collection5/1132.txt    \n","  inflating: Collection5/1133.ann    \n","  inflating: Collection5/1133.txt    \n","  inflating: Collection5/1134.ann    \n","  inflating: Collection5/1134.txt    \n","  inflating: Collection5/1135.ann    \n","  inflating: Collection5/1135.txt    \n","  inflating: Collection5/1136.ann    \n","  inflating: Collection5/1136.txt    \n","  inflating: Collection5/1137.ann    \n","  inflating: Collection5/1137.txt    \n","  inflating: Collection5/1138.ann    \n","  inflating: Collection5/1138.txt    \n","  inflating: Collection5/1139.ann    \n","  inflating: Collection5/1139.txt    \n","  inflating: Collection5/114.ann     \n","  inflating: Collection5/114.txt     \n","  inflating: Collection5/1140.ann    \n","  inflating: Collection5/1140.txt    \n","  inflating: Collection5/1141.ann    \n","  inflating: Collection5/1141.txt    \n","  inflating: Collection5/1142.ann    \n","  inflating: Collection5/1142.txt    \n","  inflating: Collection5/1143.ann    \n","  inflating: Collection5/1143.txt    \n","  inflating: Collection5/1144.ann    \n","  inflating: Collection5/1144.txt    \n","  inflating: Collection5/1145.ann    \n","  inflating: Collection5/1145.txt    \n","  inflating: Collection5/1146.ann    \n","  inflating: Collection5/1146.txt    \n","  inflating: Collection5/1147.ann    \n","  inflating: Collection5/1147.txt    \n","  inflating: Collection5/1148.ann    \n","  inflating: Collection5/1148.txt    \n","  inflating: Collection5/1149.ann    \n","  inflating: Collection5/1149.txt    \n","  inflating: Collection5/115.ann     \n","  inflating: Collection5/115.txt     \n","  inflating: Collection5/1150.ann    \n","  inflating: Collection5/1150.txt    \n","  inflating: Collection5/1151.ann    \n","  inflating: Collection5/1151.txt    \n","  inflating: Collection5/1152.ann    \n","  inflating: Collection5/1152.txt    \n","  inflating: Collection5/1153.ann    \n","  inflating: Collection5/1153.txt    \n","  inflating: Collection5/1154.ann    \n","  inflating: Collection5/1154.txt    \n","  inflating: Collection5/1155.ann    \n","  inflating: Collection5/1155.txt    \n","  inflating: Collection5/1156.ann    \n","  inflating: Collection5/1156.txt    \n","  inflating: Collection5/1157.ann    \n","  inflating: Collection5/1157.txt    \n","  inflating: Collection5/1158.ann    \n","  inflating: Collection5/1158.txt    \n","  inflating: Collection5/1159.ann    \n","  inflating: Collection5/1159.txt    \n","  inflating: Collection5/116.ann     \n","  inflating: Collection5/116.txt     \n","  inflating: Collection5/1160.ann    \n","  inflating: Collection5/1160.txt    \n","  inflating: Collection5/1161.ann    \n","  inflating: Collection5/1161.txt    \n","  inflating: Collection5/1162.ann    \n","  inflating: Collection5/1162.txt    \n","  inflating: Collection5/1163.ann    \n","  inflating: Collection5/1163.txt    \n","  inflating: Collection5/1164.ann    \n","  inflating: Collection5/1164.txt    \n","  inflating: Collection5/1165.ann    \n","  inflating: Collection5/1165.txt    \n","  inflating: Collection5/1166.ann    \n","  inflating: Collection5/1166.txt    \n","  inflating: Collection5/1167.ann    \n","  inflating: Collection5/1167.txt    \n","  inflating: Collection5/1168.ann    \n","  inflating: Collection5/1168.txt    \n","  inflating: Collection5/1169.ann    \n","  inflating: Collection5/1169.txt    \n","  inflating: Collection5/117.ann     \n","  inflating: Collection5/117.txt     \n","  inflating: Collection5/1170.ann    \n","  inflating: Collection5/1170.txt    \n","  inflating: Collection5/1171.ann    \n","  inflating: Collection5/1171.txt    \n","  inflating: Collection5/1172.ann    \n","  inflating: Collection5/1172.txt    \n","  inflating: Collection5/1173.ann    \n","  inflating: Collection5/1173.txt    \n","  inflating: Collection5/1174.ann    \n","  inflating: Collection5/1174.txt    \n","  inflating: Collection5/1175.ann    \n","  inflating: Collection5/1175.txt    \n","  inflating: Collection5/1176.ann    \n","  inflating: Collection5/1176.txt    \n","  inflating: Collection5/1177.ann    \n","  inflating: Collection5/1177.txt    \n","  inflating: Collection5/1178.ann    \n","  inflating: Collection5/1178.txt    \n","  inflating: Collection5/1179.ann    \n","  inflating: Collection5/1179.txt    \n","  inflating: Collection5/118.ann     \n","  inflating: Collection5/118.txt     \n","  inflating: Collection5/1180.ann    \n","  inflating: Collection5/1180.txt    \n","  inflating: Collection5/1181.ann    \n","  inflating: Collection5/1181.txt    \n","  inflating: Collection5/1182.ann    \n","  inflating: Collection5/1182.txt    \n","  inflating: Collection5/1183.ann    \n","  inflating: Collection5/1183.txt    \n","  inflating: Collection5/1184.ann    \n","  inflating: Collection5/1184.txt    \n","  inflating: Collection5/1185.ann    \n","  inflating: Collection5/1185.txt    \n","  inflating: Collection5/1186.ann    \n","  inflating: Collection5/1186.txt    \n","  inflating: Collection5/1187.ann    \n","  inflating: Collection5/1187.txt    \n","  inflating: Collection5/1188.ann    \n","  inflating: Collection5/1188.txt    \n","  inflating: Collection5/1189.ann    \n","  inflating: Collection5/1189.txt    \n","  inflating: Collection5/119.ann     \n","  inflating: Collection5/119.txt     \n","  inflating: Collection5/1190.ann    \n","  inflating: Collection5/1190.txt    \n","  inflating: Collection5/1191.ann    \n","  inflating: Collection5/1191.txt    \n","  inflating: Collection5/1192.ann    \n","  inflating: Collection5/1192.txt    \n","  inflating: Collection5/1193.ann    \n","  inflating: Collection5/1193.txt    \n","  inflating: Collection5/1194.ann    \n","  inflating: Collection5/1194.txt    \n","  inflating: Collection5/1195.ann    \n","  inflating: Collection5/1195.txt    \n","  inflating: Collection5/1196.ann    \n","  inflating: Collection5/1196.txt    \n","  inflating: Collection5/1197.ann    \n","  inflating: Collection5/1197.txt    \n","  inflating: Collection5/1198.ann    \n","  inflating: Collection5/1198.txt    \n","  inflating: Collection5/1199.ann    \n","  inflating: Collection5/1199.txt    \n","  inflating: Collection5/11_01_13b.ann  \n","  inflating: Collection5/11_01_13b.txt  \n","  inflating: Collection5/11_01_13e.ann  \n","  inflating: Collection5/11_01_13e.txt  \n","  inflating: Collection5/120.ann     \n","  inflating: Collection5/120.txt     \n","  inflating: Collection5/1200.ann    \n","  inflating: Collection5/1200.txt    \n","  inflating: Collection5/121.ann     \n","  inflating: Collection5/121.txt     \n","  inflating: Collection5/122.ann     \n","  inflating: Collection5/122.txt     \n","  inflating: Collection5/123.ann     \n","  inflating: Collection5/123.txt     \n","  inflating: Collection5/124.ann     \n","  inflating: Collection5/124.txt     \n","  inflating: Collection5/125.ann     \n","  inflating: Collection5/125.txt     \n","  inflating: Collection5/126.ann     \n","  inflating: Collection5/126.txt     \n","  inflating: Collection5/127.ann     \n","  inflating: Collection5/127.txt     \n","  inflating: Collection5/128.ann     \n","  inflating: Collection5/128.txt     \n","  inflating: Collection5/129.ann     \n","  inflating: Collection5/129.txt     \n","  inflating: Collection5/130.ann     \n","  inflating: Collection5/130.txt     \n","  inflating: Collection5/131.ann     \n","  inflating: Collection5/131.txt     \n","  inflating: Collection5/132.ann     \n","  inflating: Collection5/132.txt     \n","  inflating: Collection5/133.ann     \n","  inflating: Collection5/133.txt     \n","  inflating: Collection5/134.ann     \n","  inflating: Collection5/134.txt     \n","  inflating: Collection5/135.ann     \n","  inflating: Collection5/135.txt     \n","  inflating: Collection5/136.ann     \n","  inflating: Collection5/136.txt     \n","  inflating: Collection5/137.ann     \n","  inflating: Collection5/137.txt     \n","  inflating: Collection5/138.ann     \n","  inflating: Collection5/138.txt     \n","  inflating: Collection5/139.ann     \n","  inflating: Collection5/139.txt     \n","  inflating: Collection5/140.ann     \n","  inflating: Collection5/140.txt     \n","  inflating: Collection5/141.ann     \n","  inflating: Collection5/141.txt     \n","  inflating: Collection5/142.ann     \n","  inflating: Collection5/142.txt     \n","  inflating: Collection5/143.ann     \n","  inflating: Collection5/143.txt     \n","  inflating: Collection5/144.ann     \n","  inflating: Collection5/144.txt     \n","  inflating: Collection5/145.ann     \n","  inflating: Collection5/145.txt     \n","  inflating: Collection5/146.ann     \n","  inflating: Collection5/146.txt     \n","  inflating: Collection5/147.ann     \n","  inflating: Collection5/147.txt     \n","  inflating: Collection5/148.ann     \n","  inflating: Collection5/148.txt     \n","  inflating: Collection5/149.ann     \n","  inflating: Collection5/149.txt     \n","  inflating: Collection5/14_01_13c.ann  \n","  inflating: Collection5/14_01_13c.txt  \n","  inflating: Collection5/14_01_13g.ann  \n","  inflating: Collection5/14_01_13g.txt  \n","  inflating: Collection5/14_01_13i.ann  \n","  inflating: Collection5/14_01_13i.txt  \n","  inflating: Collection5/150.ann     \n","  inflating: Collection5/150.txt     \n","  inflating: Collection5/151.ann     \n","  inflating: Collection5/151.txt     \n","  inflating: Collection5/152.ann     \n","  inflating: Collection5/152.txt     \n","  inflating: Collection5/153.ann     \n","  inflating: Collection5/153.txt     \n","  inflating: Collection5/154.ann     \n","  inflating: Collection5/154.txt     \n","  inflating: Collection5/155.ann     \n","  inflating: Collection5/155.txt     \n","  inflating: Collection5/156.ann     \n","  inflating: Collection5/156.txt     \n","  inflating: Collection5/157.ann     \n","  inflating: Collection5/157.txt     \n","  inflating: Collection5/158.ann     \n","  inflating: Collection5/158.txt     \n","  inflating: Collection5/159.ann     \n","  inflating: Collection5/159.txt     \n","  inflating: Collection5/15_01_13a.ann  \n","  inflating: Collection5/15_01_13a.txt  \n","  inflating: Collection5/15_01_13b.ann  \n","  inflating: Collection5/15_01_13b.txt  \n","  inflating: Collection5/15_01_13e.ann  \n","  inflating: Collection5/15_01_13e.txt  \n","  inflating: Collection5/15_01_13f.ann  \n","  inflating: Collection5/15_01_13f.txt  \n","  inflating: Collection5/160.ann     \n","  inflating: Collection5/160.txt     \n","  inflating: Collection5/161.ann     \n","  inflating: Collection5/161.txt     \n","  inflating: Collection5/162.ann     \n","  inflating: Collection5/162.txt     \n","  inflating: Collection5/163.ann     \n","  inflating: Collection5/163.txt     \n","  inflating: Collection5/164.ann     \n","  inflating: Collection5/164.txt     \n","  inflating: Collection5/165.ann     \n","  inflating: Collection5/165.txt     \n","  inflating: Collection5/166.ann     \n","  inflating: Collection5/166.txt     \n","  inflating: Collection5/167.ann     \n","  inflating: Collection5/167.txt     \n","  inflating: Collection5/168.ann     \n","  inflating: Collection5/168.txt     \n","  inflating: Collection5/169.ann     \n","  inflating: Collection5/169.txt     \n","  inflating: Collection5/170.ann     \n","  inflating: Collection5/170.txt     \n","  inflating: Collection5/171.ann     \n","  inflating: Collection5/171.txt     \n","  inflating: Collection5/172.ann     \n","  inflating: Collection5/172.txt     \n","  inflating: Collection5/173.ann     \n","  inflating: Collection5/173.txt     \n","  inflating: Collection5/174.ann     \n","  inflating: Collection5/174.txt     \n","  inflating: Collection5/175.ann     \n","  inflating: Collection5/175.txt     \n","  inflating: Collection5/176.ann     \n","  inflating: Collection5/176.txt     \n","  inflating: Collection5/177.ann     \n","  inflating: Collection5/177.txt     \n","  inflating: Collection5/178.ann     \n","  inflating: Collection5/178.txt     \n","  inflating: Collection5/179.ann     \n","  inflating: Collection5/179.txt     \n","  inflating: Collection5/180.ann     \n","  inflating: Collection5/180.txt     \n","  inflating: Collection5/181.ann     \n","  inflating: Collection5/181.txt     \n","  inflating: Collection5/182.ann     \n","  inflating: Collection5/182.txt     \n","  inflating: Collection5/183.ann     \n","  inflating: Collection5/183.txt     \n","  inflating: Collection5/184.ann     \n","  inflating: Collection5/184.txt     \n","  inflating: Collection5/185.ann     \n","  inflating: Collection5/185.txt     \n","  inflating: Collection5/186.ann     \n","  inflating: Collection5/186.txt     \n","  inflating: Collection5/187.ann     \n","  inflating: Collection5/187.txt     \n","  inflating: Collection5/188.ann     \n","  inflating: Collection5/188.txt     \n","  inflating: Collection5/189.ann     \n","  inflating: Collection5/189.txt     \n","  inflating: Collection5/190.ann     \n","  inflating: Collection5/190.txt     \n","  inflating: Collection5/191.ann     \n","  inflating: Collection5/191.txt     \n","  inflating: Collection5/192.ann     \n","  inflating: Collection5/192.txt     \n","  inflating: Collection5/193.ann     \n","  inflating: Collection5/193.txt     \n","  inflating: Collection5/194.ann     \n","  inflating: Collection5/194.txt     \n","  inflating: Collection5/195.ann     \n","  inflating: Collection5/195.txt     \n","  inflating: Collection5/196.ann     \n","  inflating: Collection5/196.txt     \n","  inflating: Collection5/197.ann     \n","  inflating: Collection5/197.txt     \n","  inflating: Collection5/198.ann     \n","  inflating: Collection5/198.txt     \n","  inflating: Collection5/199.ann     \n","  inflating: Collection5/199.txt     \n","  inflating: Collection5/19_11_12d.ann  \n","  inflating: Collection5/19_11_12d.txt  \n","  inflating: Collection5/19_11_12h.ann  \n","  inflating: Collection5/19_11_12h.txt  \n","  inflating: Collection5/200.ann     \n","  inflating: Collection5/200.txt     \n","  inflating: Collection5/2001.ann    \n","  inflating: Collection5/2001.txt    \n","  inflating: Collection5/2002.ann    \n","  inflating: Collection5/2002.txt    \n","  inflating: Collection5/2003.ann    \n","  inflating: Collection5/2003.txt    \n","  inflating: Collection5/2004.ann    \n","  inflating: Collection5/2004.txt    \n","  inflating: Collection5/2005.ann    \n","  inflating: Collection5/2005.txt    \n","  inflating: Collection5/2006.ann    \n","  inflating: Collection5/2006.txt    \n","  inflating: Collection5/2007.ann    \n","  inflating: Collection5/2007.txt    \n","  inflating: Collection5/2008.ann    \n","  inflating: Collection5/2008.txt    \n","  inflating: Collection5/2009.ann    \n","  inflating: Collection5/2009.txt    \n","  inflating: Collection5/201.ann     \n","  inflating: Collection5/201.txt     \n","  inflating: Collection5/2010.ann    \n","  inflating: Collection5/2010.txt    \n","  inflating: Collection5/2011.ann    \n","  inflating: Collection5/2011.txt    \n","  inflating: Collection5/2012.ann    \n","  inflating: Collection5/2012.txt    \n","  inflating: Collection5/2013.ann    \n","  inflating: Collection5/2013.txt    \n","  inflating: Collection5/2014.ann    \n","  inflating: Collection5/2014.txt    \n","  inflating: Collection5/2015.ann    \n","  inflating: Collection5/2015.txt    \n","  inflating: Collection5/2016.ann    \n","  inflating: Collection5/2016.txt    \n","  inflating: Collection5/2017.ann    \n","  inflating: Collection5/2017.txt    \n","  inflating: Collection5/2018.ann    \n","  inflating: Collection5/2018.txt    \n","  inflating: Collection5/2019.ann    \n","  inflating: Collection5/2019.txt    \n","  inflating: Collection5/202.ann     \n","  inflating: Collection5/202.txt     \n","  inflating: Collection5/2020.ann    \n","  inflating: Collection5/2020.txt    \n","  inflating: Collection5/2021.ann    \n","  inflating: Collection5/2021.txt    \n","  inflating: Collection5/2022.ann    \n","  inflating: Collection5/2022.txt    \n","  inflating: Collection5/2023.ann    \n","  inflating: Collection5/2023.txt    \n","  inflating: Collection5/2024.ann    \n","  inflating: Collection5/2024.txt    \n","  inflating: Collection5/2025.ann    \n","  inflating: Collection5/2025.txt    \n","  inflating: Collection5/2026.ann    \n","  inflating: Collection5/2026.txt    \n","  inflating: Collection5/2027.ann    \n","  inflating: Collection5/2027.txt    \n","  inflating: Collection5/2028.ann    \n","  inflating: Collection5/2028.txt    \n","  inflating: Collection5/2029.ann    \n","  inflating: Collection5/2029.txt    \n","  inflating: Collection5/203.ann     \n","  inflating: Collection5/203.txt     \n","  inflating: Collection5/2030.ann    \n","  inflating: Collection5/2030.txt    \n","  inflating: Collection5/2031.ann    \n","  inflating: Collection5/2031.txt    \n","  inflating: Collection5/2032.ann    \n","  inflating: Collection5/2032.txt    \n","  inflating: Collection5/2034.ann    \n","  inflating: Collection5/2034.txt    \n","  inflating: Collection5/2035.ann    \n","  inflating: Collection5/2035.txt    \n","  inflating: Collection5/2036.ann    \n","  inflating: Collection5/2036.txt    \n","  inflating: Collection5/2037.ann    \n","  inflating: Collection5/2037.txt    \n","  inflating: Collection5/2038.ann    \n","  inflating: Collection5/2038.txt    \n","  inflating: Collection5/2039.ann    \n","  inflating: Collection5/2039.txt    \n","  inflating: Collection5/204.ann     \n","  inflating: Collection5/204.txt     \n","  inflating: Collection5/2040.ann    \n","  inflating: Collection5/2040.txt    \n","  inflating: Collection5/2041.ann    \n","  inflating: Collection5/2041.txt    \n","  inflating: Collection5/2042.ann    \n","  inflating: Collection5/2042.txt    \n","  inflating: Collection5/2043.ann    \n","  inflating: Collection5/2043.txt    \n","  inflating: Collection5/2044.ann    \n","  inflating: Collection5/2044.txt    \n","  inflating: Collection5/2045.ann    \n","  inflating: Collection5/2045.txt    \n","  inflating: Collection5/2046.ann    \n","  inflating: Collection5/2046.txt    \n","  inflating: Collection5/2047.ann    \n","  inflating: Collection5/2047.txt    \n","  inflating: Collection5/2048.ann    \n","  inflating: Collection5/2048.txt    \n","  inflating: Collection5/2049.ann    \n","  inflating: Collection5/2049.txt    \n","  inflating: Collection5/205.ann     \n","  inflating: Collection5/205.txt     \n","  inflating: Collection5/2050.ann    \n","  inflating: Collection5/2050.txt    \n","  inflating: Collection5/206.ann     \n","  inflating: Collection5/206.txt     \n","  inflating: Collection5/207.ann     \n","  inflating: Collection5/207.txt     \n","  inflating: Collection5/208.ann     \n","  inflating: Collection5/208.txt     \n","  inflating: Collection5/209.ann     \n","  inflating: Collection5/209.txt     \n","  inflating: Collection5/20_11_12a.ann  \n","  inflating: Collection5/20_11_12a.txt  \n","  inflating: Collection5/20_11_12b.ann  \n","  inflating: Collection5/20_11_12b.txt  \n","  inflating: Collection5/20_11_12c.ann  \n","  inflating: Collection5/20_11_12c.txt  \n","  inflating: Collection5/20_11_12d.ann  \n","  inflating: Collection5/20_11_12d.txt  \n","  inflating: Collection5/20_11_12i.ann  \n","  inflating: Collection5/20_11_12i.txt  \n","  inflating: Collection5/210.ann     \n","  inflating: Collection5/210.txt     \n","  inflating: Collection5/211.ann     \n","  inflating: Collection5/211.txt     \n","  inflating: Collection5/212.ann     \n","  inflating: Collection5/212.txt     \n","  inflating: Collection5/213.ann     \n","  inflating: Collection5/213.txt     \n","  inflating: Collection5/214.ann     \n","  inflating: Collection5/214.txt     \n","  inflating: Collection5/215.ann     \n","  inflating: Collection5/215.txt     \n","  inflating: Collection5/216.ann     \n","  inflating: Collection5/216.txt     \n","  inflating: Collection5/217.ann     \n","  inflating: Collection5/217.txt     \n","  inflating: Collection5/218.ann     \n","  inflating: Collection5/218.txt     \n","  inflating: Collection5/219.ann     \n","  inflating: Collection5/219.txt     \n","  inflating: Collection5/21_11_12c.ann  \n","  inflating: Collection5/21_11_12c.txt  \n","  inflating: Collection5/21_11_12h.ann  \n","  inflating: Collection5/21_11_12h.txt  \n","  inflating: Collection5/21_11_12i.ann  \n","  inflating: Collection5/21_11_12i.txt  \n","  inflating: Collection5/21_11_12j.ann  \n","  inflating: Collection5/21_11_12j.txt  \n","  inflating: Collection5/220.ann     \n","  inflating: Collection5/220.txt     \n","  inflating: Collection5/221.ann     \n","  inflating: Collection5/221.txt     \n","  inflating: Collection5/222.ann     \n","  inflating: Collection5/222.txt     \n","  inflating: Collection5/223.ann     \n","  inflating: Collection5/223.txt     \n","  inflating: Collection5/224.ann     \n","  inflating: Collection5/224.txt     \n","  inflating: Collection5/225.ann     \n","  inflating: Collection5/225.txt     \n","  inflating: Collection5/226.ann     \n","  inflating: Collection5/226.txt     \n","  inflating: Collection5/227.ann     \n","  inflating: Collection5/227.txt     \n","  inflating: Collection5/228.ann     \n","  inflating: Collection5/228.txt     \n","  inflating: Collection5/229.ann     \n","  inflating: Collection5/229.txt     \n","  inflating: Collection5/22_11_12a.ann  \n","  inflating: Collection5/22_11_12a.txt  \n","  inflating: Collection5/22_11_12c.ann  \n","  inflating: Collection5/22_11_12c.txt  \n","  inflating: Collection5/22_11_12d.ann  \n","  inflating: Collection5/22_11_12d.txt  \n","  inflating: Collection5/22_11_12g.ann  \n","  inflating: Collection5/22_11_12g.txt  \n","  inflating: Collection5/22_11_12h.ann  \n","  inflating: Collection5/22_11_12h.txt  \n","  inflating: Collection5/22_11_12i.ann  \n","  inflating: Collection5/22_11_12i.txt  \n","  inflating: Collection5/22_11_12j.ann  \n","  inflating: Collection5/22_11_12j.txt  \n","  inflating: Collection5/230.ann     \n","  inflating: Collection5/230.txt     \n","  inflating: Collection5/231.ann     \n","  inflating: Collection5/231.txt     \n","  inflating: Collection5/232.ann     \n","  inflating: Collection5/232.txt     \n","  inflating: Collection5/233.ann     \n","  inflating: Collection5/233.txt     \n","  inflating: Collection5/234.ann     \n","  inflating: Collection5/234.txt     \n","  inflating: Collection5/235.ann     \n","  inflating: Collection5/235.txt     \n","  inflating: Collection5/236.ann     \n","  inflating: Collection5/236.txt     \n","  inflating: Collection5/237.ann     \n","  inflating: Collection5/237.txt     \n","  inflating: Collection5/238.ann     \n","  inflating: Collection5/238.txt     \n","  inflating: Collection5/239.ann     \n","  inflating: Collection5/239.txt     \n","  inflating: Collection5/23_11_12a.ann  \n","  inflating: Collection5/23_11_12a.txt  \n","  inflating: Collection5/23_11_12b.ann  \n","  inflating: Collection5/23_11_12b.txt  \n","  inflating: Collection5/23_11_12c.ann  \n","  inflating: Collection5/23_11_12c.txt  \n","  inflating: Collection5/23_11_12d.ann  \n","  inflating: Collection5/23_11_12d.txt  \n","  inflating: Collection5/23_11_12e.ann  \n","  inflating: Collection5/23_11_12e.txt  \n","  inflating: Collection5/23_11_12f.ann  \n","  inflating: Collection5/23_11_12f.txt  \n","  inflating: Collection5/240.ann     \n","  inflating: Collection5/240.txt     \n","  inflating: Collection5/241.ann     \n","  inflating: Collection5/241.txt     \n","  inflating: Collection5/242.ann     \n","  inflating: Collection5/242.txt     \n","  inflating: Collection5/243.ann     \n","  inflating: Collection5/243.txt     \n","  inflating: Collection5/244.ann     \n","  inflating: Collection5/244.txt     \n","  inflating: Collection5/245.ann     \n","  inflating: Collection5/245.txt     \n","  inflating: Collection5/246.ann     \n","  inflating: Collection5/246.txt     \n","  inflating: Collection5/247.ann     \n","  inflating: Collection5/247.txt     \n","  inflating: Collection5/248.ann     \n","  inflating: Collection5/248.txt     \n","  inflating: Collection5/249.ann     \n","  inflating: Collection5/249.txt     \n","  inflating: Collection5/250.ann     \n","  inflating: Collection5/250.txt     \n","  inflating: Collection5/251.ann     \n","  inflating: Collection5/251.txt     \n","  inflating: Collection5/252.ann     \n","  inflating: Collection5/252.txt     \n","  inflating: Collection5/253.ann     \n","  inflating: Collection5/253.txt     \n","  inflating: Collection5/254.ann     \n","  inflating: Collection5/254.txt     \n","  inflating: Collection5/255.ann     \n","  inflating: Collection5/255.txt     \n","  inflating: Collection5/256.ann     \n","  inflating: Collection5/256.txt     \n","  inflating: Collection5/257.ann     \n","  inflating: Collection5/257.txt     \n","  inflating: Collection5/258.ann     \n","  inflating: Collection5/258.txt     \n","  inflating: Collection5/259.ann     \n","  inflating: Collection5/259.txt     \n","  inflating: Collection5/25_12_12a.ann  \n","  inflating: Collection5/25_12_12a.txt  \n","  inflating: Collection5/25_12_12c.ann  \n","  inflating: Collection5/25_12_12c.txt  \n","  inflating: Collection5/25_12_12d.ann  \n","  inflating: Collection5/25_12_12d.txt  \n","  inflating: Collection5/25_12_12e.ann  \n","  inflating: Collection5/25_12_12e.txt  \n","  inflating: Collection5/260.ann     \n","  inflating: Collection5/260.txt     \n","  inflating: Collection5/261.ann     \n","  inflating: Collection5/261.txt     \n","  inflating: Collection5/262.ann     \n","  inflating: Collection5/262.txt     \n","  inflating: Collection5/263.ann     \n","  inflating: Collection5/263.txt     \n","  inflating: Collection5/264.ann     \n","  inflating: Collection5/264.txt     \n","  inflating: Collection5/265.ann     \n","  inflating: Collection5/265.txt     \n","  inflating: Collection5/266.ann     \n","  inflating: Collection5/266.txt     \n","  inflating: Collection5/267.ann     \n","  inflating: Collection5/267.txt     \n","  inflating: Collection5/268.ann     \n","  inflating: Collection5/268.txt     \n","  inflating: Collection5/269.ann     \n","  inflating: Collection5/269.txt     \n","  inflating: Collection5/26_11_12b.ann  \n","  inflating: Collection5/26_11_12b.txt  \n","  inflating: Collection5/26_11_12c.ann  \n","  inflating: Collection5/26_11_12c.txt  \n","  inflating: Collection5/26_11_12e.ann  \n","  inflating: Collection5/26_11_12e.txt  \n","  inflating: Collection5/26_11_12f.ann  \n","  inflating: Collection5/26_11_12f.txt  \n","  inflating: Collection5/270.ann     \n","  inflating: Collection5/270.txt     \n","  inflating: Collection5/271.ann     \n","  inflating: Collection5/271.txt     \n","  inflating: Collection5/272.ann     \n","  inflating: Collection5/272.txt     \n","  inflating: Collection5/273.ann     \n","  inflating: Collection5/273.txt     \n","  inflating: Collection5/274.ann     \n","  inflating: Collection5/274.txt     \n","  inflating: Collection5/275.ann     \n","  inflating: Collection5/275.txt     \n","  inflating: Collection5/276.ann     \n","  inflating: Collection5/276.txt     \n","  inflating: Collection5/277.ann     \n","  inflating: Collection5/277.txt     \n","  inflating: Collection5/278.ann     \n","  inflating: Collection5/278.txt     \n","  inflating: Collection5/279.ann     \n","  inflating: Collection5/279.txt     \n","  inflating: Collection5/27_11_12a.ann  \n","  inflating: Collection5/27_11_12a.txt  \n","  inflating: Collection5/27_11_12c.ann  \n","  inflating: Collection5/27_11_12c.txt  \n","  inflating: Collection5/27_11_12d.ann  \n","  inflating: Collection5/27_11_12d.txt  \n","  inflating: Collection5/27_11_12e.ann  \n","  inflating: Collection5/27_11_12e.txt  \n","  inflating: Collection5/27_11_12j.ann  \n","  inflating: Collection5/27_11_12j.txt  \n","  inflating: Collection5/280.ann     \n","  inflating: Collection5/280.txt     \n","  inflating: Collection5/281.ann     \n","  inflating: Collection5/281.txt     \n","  inflating: Collection5/282.ann     \n","  inflating: Collection5/282.txt     \n","  inflating: Collection5/283.ann     \n","  inflating: Collection5/283.txt     \n","  inflating: Collection5/284.ann     \n","  inflating: Collection5/284.txt     \n","  inflating: Collection5/285.ann     \n","  inflating: Collection5/285.txt     \n","  inflating: Collection5/286.ann     \n","  inflating: Collection5/286.txt     \n","  inflating: Collection5/287.ann     \n","  inflating: Collection5/287.txt     \n","  inflating: Collection5/288.ann     \n","  inflating: Collection5/288.txt     \n","  inflating: Collection5/289.ann     \n","  inflating: Collection5/289.txt     \n","  inflating: Collection5/28_11_12a.ann  \n","  inflating: Collection5/28_11_12a.txt  \n","  inflating: Collection5/28_11_12f.ann  \n","  inflating: Collection5/28_11_12f.txt  \n","  inflating: Collection5/28_11_12g.ann  \n","  inflating: Collection5/28_11_12g.txt  \n","  inflating: Collection5/28_11_12h.ann  \n","  inflating: Collection5/28_11_12h.txt  \n","  inflating: Collection5/28_11_12i.ann  \n","  inflating: Collection5/28_11_12i.txt  \n","  inflating: Collection5/28_11_12j.ann  \n","  inflating: Collection5/28_11_12j.txt  \n","  inflating: Collection5/290.ann     \n","  inflating: Collection5/290.txt     \n","  inflating: Collection5/291.ann     \n","  inflating: Collection5/291.txt     \n","  inflating: Collection5/292.ann     \n","  inflating: Collection5/292.txt     \n","  inflating: Collection5/293.ann     \n","  inflating: Collection5/293.txt     \n","  inflating: Collection5/294.ann     \n","  inflating: Collection5/294.txt     \n","  inflating: Collection5/295.ann     \n","  inflating: Collection5/295.txt     \n","  inflating: Collection5/296.ann     \n","  inflating: Collection5/296.txt     \n","  inflating: Collection5/297.ann     \n","  inflating: Collection5/297.txt     \n","  inflating: Collection5/298.ann     \n","  inflating: Collection5/298.txt     \n","  inflating: Collection5/299.ann     \n","  inflating: Collection5/299.txt     \n","  inflating: Collection5/29_11_12a.ann  \n","  inflating: Collection5/29_11_12a.txt  \n","  inflating: Collection5/29_11_12b.ann  \n","  inflating: Collection5/29_11_12b.txt  \n","  inflating: Collection5/300.ann     \n","  inflating: Collection5/300.txt     \n","  inflating: Collection5/301.ann     \n","  inflating: Collection5/301.txt     \n","  inflating: Collection5/302.ann     \n","  inflating: Collection5/302.txt     \n","  inflating: Collection5/303.ann     \n","  inflating: Collection5/303.txt     \n","  inflating: Collection5/304.ann     \n","  inflating: Collection5/304.txt     \n","  inflating: Collection5/305.ann     \n","  inflating: Collection5/305.txt     \n","  inflating: Collection5/306.ann     \n","  inflating: Collection5/306.txt     \n","  inflating: Collection5/307.ann     \n","  inflating: Collection5/307.txt     \n","  inflating: Collection5/308.ann     \n","  inflating: Collection5/308.txt     \n","  inflating: Collection5/309.ann     \n","  inflating: Collection5/309.txt     \n","  inflating: Collection5/30_11_12b.ann  \n","  inflating: Collection5/30_11_12b.txt  \n","  inflating: Collection5/30_11_12h.ann  \n","  inflating: Collection5/30_11_12h.txt  \n","  inflating: Collection5/30_11_12i.ann  \n","  inflating: Collection5/30_11_12i.txt  \n","  inflating: Collection5/310.ann     \n","  inflating: Collection5/310.txt     \n","  inflating: Collection5/311.ann     \n","  inflating: Collection5/311.txt     \n","  inflating: Collection5/312.ann     \n","  inflating: Collection5/312.txt     \n","  inflating: Collection5/313.ann     \n","  inflating: Collection5/313.txt     \n","  inflating: Collection5/314.ann     \n","  inflating: Collection5/314.txt     \n","  inflating: Collection5/315.ann     \n","  inflating: Collection5/315.txt     \n","  inflating: Collection5/316.ann     \n","  inflating: Collection5/316.txt     \n","  inflating: Collection5/317.ann     \n","  inflating: Collection5/317.txt     \n","  inflating: Collection5/318.ann     \n","  inflating: Collection5/318.txt     \n","  inflating: Collection5/319.ann     \n","  inflating: Collection5/319.txt     \n","  inflating: Collection5/320.ann     \n","  inflating: Collection5/320.txt     \n","  inflating: Collection5/321.ann     \n","  inflating: Collection5/321.txt     \n","  inflating: Collection5/322.ann     \n","  inflating: Collection5/322.txt     \n","  inflating: Collection5/323.ann     \n","  inflating: Collection5/323.txt     \n","  inflating: Collection5/324.ann     \n","  inflating: Collection5/324.txt     \n","  inflating: Collection5/325.ann     \n","  inflating: Collection5/325.txt     \n","  inflating: Collection5/326.ann     \n","  inflating: Collection5/326.txt     \n","  inflating: Collection5/327.ann     \n","  inflating: Collection5/327.txt     \n","  inflating: Collection5/328.ann     \n","  inflating: Collection5/328.txt     \n","  inflating: Collection5/329.ann     \n","  inflating: Collection5/329.txt     \n","  inflating: Collection5/330.ann     \n","  inflating: Collection5/330.txt     \n","  inflating: Collection5/331.ann     \n","  inflating: Collection5/331.txt     \n","  inflating: Collection5/332.ann     \n","  inflating: Collection5/332.txt     \n","  inflating: Collection5/333.ann     \n","  inflating: Collection5/333.txt     \n","  inflating: Collection5/334.ann     \n","  inflating: Collection5/334.txt     \n","  inflating: Collection5/335.ann     \n","  inflating: Collection5/335.txt     \n","  inflating: Collection5/336.ann     \n","  inflating: Collection5/336.txt     \n","  inflating: Collection5/337.ann     \n","  inflating: Collection5/337.txt     \n","  inflating: Collection5/338.ann     \n","  inflating: Collection5/338.txt     \n","  inflating: Collection5/339.ann     \n","  inflating: Collection5/339.txt     \n","  inflating: Collection5/340.ann     \n","  inflating: Collection5/340.txt     \n","  inflating: Collection5/341.ann     \n","  inflating: Collection5/341.txt     \n","  inflating: Collection5/342.ann     \n","  inflating: Collection5/342.txt     \n","  inflating: Collection5/343.ann     \n","  inflating: Collection5/343.txt     \n","  inflating: Collection5/344.ann     \n","  inflating: Collection5/344.txt     \n","  inflating: Collection5/345.ann     \n","  inflating: Collection5/345.txt     \n","  inflating: Collection5/346.ann     \n","  inflating: Collection5/346.txt     \n","  inflating: Collection5/347.ann     \n","  inflating: Collection5/347.txt     \n","  inflating: Collection5/348.ann     \n","  inflating: Collection5/348.txt     \n","  inflating: Collection5/349.ann     \n","  inflating: Collection5/349.txt     \n","  inflating: Collection5/350.ann     \n","  inflating: Collection5/350.txt     \n","  inflating: Collection5/351.ann     \n","  inflating: Collection5/351.txt     \n","  inflating: Collection5/352.ann     \n","  inflating: Collection5/352.txt     \n","  inflating: Collection5/353.ann     \n","  inflating: Collection5/353.txt     \n","  inflating: Collection5/354.ann     \n","  inflating: Collection5/354.txt     \n","  inflating: Collection5/355.ann     \n","  inflating: Collection5/355.txt     \n","  inflating: Collection5/356.ann     \n","  inflating: Collection5/356.txt     \n","  inflating: Collection5/357.ann     \n","  inflating: Collection5/357.txt     \n","  inflating: Collection5/358.ann     \n","  inflating: Collection5/358.txt     \n","  inflating: Collection5/359.ann     \n","  inflating: Collection5/359.txt     \n","  inflating: Collection5/360.ann     \n","  inflating: Collection5/360.txt     \n","  inflating: Collection5/361.ann     \n","  inflating: Collection5/361.txt     \n","  inflating: Collection5/362.ann     \n","  inflating: Collection5/362.txt     \n","  inflating: Collection5/363.ann     \n","  inflating: Collection5/363.txt     \n","  inflating: Collection5/364.ann     \n","  inflating: Collection5/364.txt     \n","  inflating: Collection5/365.ann     \n","  inflating: Collection5/365.txt     \n","  inflating: Collection5/366.ann     \n","  inflating: Collection5/366.txt     \n","  inflating: Collection5/367.ann     \n","  inflating: Collection5/367.txt     \n","  inflating: Collection5/368.ann     \n","  inflating: Collection5/368.txt     \n","  inflating: Collection5/369.ann     \n","  inflating: Collection5/369.txt     \n","  inflating: Collection5/370.ann     \n","  inflating: Collection5/370.txt     \n","  inflating: Collection5/371.ann     \n","  inflating: Collection5/371.txt     \n","  inflating: Collection5/372.ann     \n","  inflating: Collection5/372.txt     \n","  inflating: Collection5/373.ann     \n","  inflating: Collection5/373.txt     \n","  inflating: Collection5/374.ann     \n","  inflating: Collection5/374.txt     \n","  inflating: Collection5/375.ann     \n","  inflating: Collection5/375.txt     \n","  inflating: Collection5/376.ann     \n","  inflating: Collection5/376.txt     \n","  inflating: Collection5/377.ann     \n","  inflating: Collection5/377.txt     \n","  inflating: Collection5/378.ann     \n","  inflating: Collection5/378.txt     \n","  inflating: Collection5/379.ann     \n","  inflating: Collection5/379.txt     \n","  inflating: Collection5/380.ann     \n","  inflating: Collection5/380.txt     \n","  inflating: Collection5/381.ann     \n","  inflating: Collection5/381.txt     \n","  inflating: Collection5/382.ann     \n","  inflating: Collection5/382.txt     \n","  inflating: Collection5/383.ann     \n","  inflating: Collection5/383.txt     \n","  inflating: Collection5/384.ann     \n","  inflating: Collection5/384.txt     \n","  inflating: Collection5/385.ann     \n","  inflating: Collection5/385.txt     \n","  inflating: Collection5/386.ann     \n","  inflating: Collection5/386.txt     \n","  inflating: Collection5/387.ann     \n","  inflating: Collection5/387.txt     \n","  inflating: Collection5/388.ann     \n","  inflating: Collection5/388.txt     \n","  inflating: Collection5/389.ann     \n","  inflating: Collection5/389.txt     \n","  inflating: Collection5/390.ann     \n","  inflating: Collection5/390.txt     \n","  inflating: Collection5/391.ann     \n","  inflating: Collection5/391.txt     \n","  inflating: Collection5/392.ann     \n","  inflating: Collection5/392.txt     \n","  inflating: Collection5/393.ann     \n","  inflating: Collection5/393.txt     \n","  inflating: Collection5/394.ann     \n","  inflating: Collection5/394.txt     \n","  inflating: Collection5/395.ann     \n","  inflating: Collection5/395.txt     \n","  inflating: Collection5/396.ann     \n","  inflating: Collection5/396.txt     \n","  inflating: Collection5/397.ann     \n","  inflating: Collection5/397.txt     \n","  inflating: Collection5/398.ann     \n","  inflating: Collection5/398.txt     \n","  inflating: Collection5/399.ann     \n","  inflating: Collection5/399.txt     \n","  inflating: Collection5/400.ann     \n","  inflating: Collection5/400.txt     \n","  inflating: Collection5/401.ann     \n","  inflating: Collection5/401.txt     \n","  inflating: Collection5/402.ann     \n","  inflating: Collection5/402.txt     \n","  inflating: Collection5/403.ann     \n","  inflating: Collection5/403.txt     \n","  inflating: Collection5/404.ann     \n","  inflating: Collection5/404.txt     \n","  inflating: Collection5/405.ann     \n","  inflating: Collection5/405.txt     \n","  inflating: Collection5/406.ann     \n","  inflating: Collection5/406.txt     \n","  inflating: Collection5/407.ann     \n","  inflating: Collection5/407.txt     \n","  inflating: Collection5/408.ann     \n","  inflating: Collection5/408.txt     \n","  inflating: Collection5/409.ann     \n","  inflating: Collection5/409.txt     \n","  inflating: Collection5/410.ann     \n","  inflating: Collection5/410.txt     \n","  inflating: Collection5/411.ann     \n","  inflating: Collection5/411.txt     \n","  inflating: Collection5/412.ann     \n","  inflating: Collection5/412.txt     \n","  inflating: Collection5/413.ann     \n","  inflating: Collection5/413.txt     \n","  inflating: Collection5/414.ann     \n","  inflating: Collection5/414.txt     \n","  inflating: Collection5/415.ann     \n","  inflating: Collection5/415.txt     \n","  inflating: Collection5/416.ann     \n","  inflating: Collection5/416.txt     \n","  inflating: Collection5/417.ann     \n","  inflating: Collection5/417.txt     \n","  inflating: Collection5/418.ann     \n","  inflating: Collection5/418.txt     \n","  inflating: Collection5/419.ann     \n","  inflating: Collection5/419.txt     \n","  inflating: Collection5/420.ann     \n","  inflating: Collection5/420.txt     \n","  inflating: Collection5/421.ann     \n","  inflating: Collection5/421.txt     \n","  inflating: Collection5/422.ann     \n","  inflating: Collection5/422.txt     \n","  inflating: Collection5/423.ann     \n","  inflating: Collection5/423.txt     \n","  inflating: Collection5/424.ann     \n","  inflating: Collection5/424.txt     \n","  inflating: Collection5/425.ann     \n","  inflating: Collection5/425.txt     \n","  inflating: Collection5/426.ann     \n","  inflating: Collection5/426.txt     \n","  inflating: Collection5/427.ann     \n","  inflating: Collection5/427.txt     \n","  inflating: Collection5/428.ann     \n","  inflating: Collection5/428.txt     \n","  inflating: Collection5/429.ann     \n","  inflating: Collection5/429.txt     \n","  inflating: Collection5/430.ann     \n","  inflating: Collection5/430.txt     \n","  inflating: Collection5/431.ann     \n","  inflating: Collection5/431.txt     \n","  inflating: Collection5/432.ann     \n","  inflating: Collection5/432.txt     \n","  inflating: Collection5/433.ann     \n","  inflating: Collection5/433.txt     \n","  inflating: Collection5/434.ann     \n","  inflating: Collection5/434.txt     \n","  inflating: Collection5/435.ann     \n","  inflating: Collection5/435.txt     \n","  inflating: Collection5/436.ann     \n","  inflating: Collection5/436.txt     \n","  inflating: Collection5/437.ann     \n","  inflating: Collection5/437.txt     \n","  inflating: Collection5/438.ann     \n","  inflating: Collection5/438.txt     \n","  inflating: Collection5/439.ann     \n","  inflating: Collection5/439.txt     \n","  inflating: Collection5/440.ann     \n","  inflating: Collection5/440.txt     \n","  inflating: Collection5/441.ann     \n","  inflating: Collection5/441.txt     \n","  inflating: Collection5/442.ann     \n","  inflating: Collection5/442.txt     \n","  inflating: Collection5/443.ann     \n","  inflating: Collection5/443.txt     \n","  inflating: Collection5/444.ann     \n","  inflating: Collection5/444.txt     \n","  inflating: Collection5/445.ann     \n","  inflating: Collection5/445.txt     \n","  inflating: Collection5/446.ann     \n","  inflating: Collection5/446.txt     \n","  inflating: Collection5/447.ann     \n","  inflating: Collection5/447.txt     \n","  inflating: Collection5/448.ann     \n","  inflating: Collection5/448.txt     \n","  inflating: Collection5/449.ann     \n","  inflating: Collection5/449.txt     \n","  inflating: Collection5/450.ann     \n","  inflating: Collection5/450.txt     \n","  inflating: Collection5/451.ann     \n","  inflating: Collection5/451.txt     \n","  inflating: Collection5/452.ann     \n","  inflating: Collection5/452.txt     \n","  inflating: Collection5/453.ann     \n","  inflating: Collection5/453.txt     \n","  inflating: Collection5/454.ann     \n","  inflating: Collection5/454.txt     \n","  inflating: Collection5/455.ann     \n","  inflating: Collection5/455.txt     \n","  inflating: Collection5/457.ann     \n","  inflating: Collection5/457.txt     \n","  inflating: Collection5/458.ann     \n","  inflating: Collection5/458.txt     \n","  inflating: Collection5/459.ann     \n","  inflating: Collection5/459.txt     \n","  inflating: Collection5/460.ann     \n","  inflating: Collection5/460.txt     \n","  inflating: Collection5/461.ann     \n","  inflating: Collection5/461.txt     \n","  inflating: Collection5/462.ann     \n","  inflating: Collection5/462.txt     \n","  inflating: Collection5/463.ann     \n","  inflating: Collection5/463.txt     \n","  inflating: Collection5/464.ann     \n","  inflating: Collection5/464.txt     \n","  inflating: Collection5/465.ann     \n","  inflating: Collection5/465.txt     \n","  inflating: Collection5/466.ann     \n","  inflating: Collection5/466.txt     \n","  inflating: Collection5/467.ann     \n","  inflating: Collection5/467.txt     \n","  inflating: Collection5/468.ann     \n","  inflating: Collection5/468.txt     \n","  inflating: Collection5/469.ann     \n","  inflating: Collection5/469.txt     \n","  inflating: Collection5/470.ann     \n","  inflating: Collection5/470.txt     \n","  inflating: Collection5/471.ann     \n","  inflating: Collection5/471.txt     \n","  inflating: Collection5/472.ann     \n","  inflating: Collection5/472.txt     \n","  inflating: Collection5/473.ann     \n","  inflating: Collection5/473.txt     \n","  inflating: Collection5/474.ann     \n","  inflating: Collection5/474.txt     \n","  inflating: Collection5/475.ann     \n","  inflating: Collection5/475.txt     \n","  inflating: Collection5/476.ann     \n","  inflating: Collection5/476.txt     \n","  inflating: Collection5/477.ann     \n","  inflating: Collection5/477.txt     \n","  inflating: Collection5/478.ann     \n","  inflating: Collection5/478.txt     \n","  inflating: Collection5/479.ann     \n","  inflating: Collection5/479.txt     \n","  inflating: Collection5/480.ann     \n","  inflating: Collection5/480.txt     \n","  inflating: Collection5/481.ann     \n","  inflating: Collection5/481.txt     \n","  inflating: Collection5/482.ann     \n","  inflating: Collection5/482.txt     \n","  inflating: Collection5/483.ann     \n","  inflating: Collection5/483.txt     \n","  inflating: Collection5/484.ann     \n","  inflating: Collection5/484.txt     \n","  inflating: Collection5/485.ann     \n","  inflating: Collection5/485.txt     \n","  inflating: Collection5/486.ann     \n","  inflating: Collection5/486.txt     \n","  inflating: Collection5/487.ann     \n","  inflating: Collection5/487.txt     \n","  inflating: Collection5/488.ann     \n","  inflating: Collection5/488.txt     \n","  inflating: Collection5/489.ann     \n","  inflating: Collection5/489.txt     \n","  inflating: Collection5/490.ann     \n","  inflating: Collection5/490.txt     \n","  inflating: Collection5/491.ann     \n","  inflating: Collection5/491.txt     \n","  inflating: Collection5/492.ann     \n","  inflating: Collection5/492.txt     \n","  inflating: Collection5/493.ann     \n","  inflating: Collection5/493.txt     \n","  inflating: Collection5/494.ann     \n","  inflating: Collection5/494.txt     \n","  inflating: Collection5/495.ann     \n","  inflating: Collection5/495.txt     \n","  inflating: Collection5/496.ann     \n","  inflating: Collection5/496.txt     \n","  inflating: Collection5/497.ann     \n","  inflating: Collection5/497.txt     \n","  inflating: Collection5/498.ann     \n","  inflating: Collection5/498.txt     \n","  inflating: Collection5/499.ann     \n","  inflating: Collection5/499.txt     \n","  inflating: Collection5/500.ann     \n","  inflating: Collection5/500.txt     \n","  inflating: Collection5/501.ann     \n","  inflating: Collection5/501.txt     \n","  inflating: Collection5/502.ann     \n","  inflating: Collection5/502.txt     \n","  inflating: Collection5/503.ann     \n","  inflating: Collection5/503.txt     \n","  inflating: Collection5/504.ann     \n","  inflating: Collection5/504.txt     \n","  inflating: Collection5/505.ann     \n","  inflating: Collection5/505.txt     \n","  inflating: Collection5/506.ann     \n","  inflating: Collection5/506.txt     \n","  inflating: Collection5/507.ann     \n","  inflating: Collection5/507.txt     \n","  inflating: Collection5/508.ann     \n","  inflating: Collection5/508.txt     \n","  inflating: Collection5/509.ann     \n","  inflating: Collection5/509.txt     \n","  inflating: Collection5/510.ann     \n","  inflating: Collection5/510.txt     \n","  inflating: Collection5/511.ann     \n","  inflating: Collection5/511.txt     \n","  inflating: Collection5/512.ann     \n","  inflating: Collection5/512.txt     \n","  inflating: Collection5/513.ann     \n","  inflating: Collection5/513.txt     \n","  inflating: Collection5/514.ann     \n","  inflating: Collection5/514.txt     \n","  inflating: Collection5/515.ann     \n","  inflating: Collection5/515.txt     \n","  inflating: Collection5/516.ann     \n","  inflating: Collection5/516.txt     \n","  inflating: Collection5/517.ann     \n","  inflating: Collection5/517.txt     \n","  inflating: Collection5/518.ann     \n","  inflating: Collection5/518.txt     \n","  inflating: Collection5/519.ann     \n","  inflating: Collection5/519.txt     \n","  inflating: Collection5/520.ann     \n","  inflating: Collection5/520.txt     \n","  inflating: Collection5/521.ann     \n","  inflating: Collection5/521.txt     \n","  inflating: Collection5/522.ann     \n","  inflating: Collection5/522.txt     \n","  inflating: Collection5/523.ann     \n","  inflating: Collection5/523.txt     \n","  inflating: Collection5/524.ann     \n","  inflating: Collection5/524.txt     \n","  inflating: Collection5/525.ann     \n","  inflating: Collection5/525.txt     \n","  inflating: Collection5/526.ann     \n","  inflating: Collection5/526.txt     \n","  inflating: Collection5/527.ann     \n","  inflating: Collection5/527.txt     \n","  inflating: Collection5/528.ann     \n","  inflating: Collection5/528.txt     \n","  inflating: Collection5/529.ann     \n","  inflating: Collection5/529.txt     \n","  inflating: Collection5/530.ann     \n","  inflating: Collection5/530.txt     \n","  inflating: Collection5/531.ann     \n","  inflating: Collection5/531.txt     \n","  inflating: Collection5/532.ann     \n","  inflating: Collection5/532.txt     \n","  inflating: Collection5/533 (!).ann  \n","  inflating: Collection5/533 (!).txt  \n","  inflating: Collection5/534.ann     \n","  inflating: Collection5/534.txt     \n","  inflating: Collection5/535.ann     \n","  inflating: Collection5/535.txt     \n","  inflating: Collection5/536.ann     \n","  inflating: Collection5/536.txt     \n","  inflating: Collection5/537.ann     \n","  inflating: Collection5/537.txt     \n","  inflating: Collection5/538.ann     \n","  inflating: Collection5/538.txt     \n","  inflating: Collection5/539.ann     \n","  inflating: Collection5/539.txt     \n","  inflating: Collection5/540.ann     \n","  inflating: Collection5/540.txt     \n","  inflating: Collection5/541.ann     \n","  inflating: Collection5/541.txt     \n","  inflating: Collection5/542.ann     \n","  inflating: Collection5/542.txt     \n","  inflating: Collection5/543.ann     \n","  inflating: Collection5/543.txt     \n","  inflating: Collection5/544.ann     \n","  inflating: Collection5/544.txt     \n","  inflating: Collection5/545.ann     \n","  inflating: Collection5/545.txt     \n","  inflating: Collection5/546.ann     \n","  inflating: Collection5/546.txt     \n","  inflating: Collection5/547.ann     \n","  inflating: Collection5/547.txt     \n","  inflating: Collection5/548.ann     \n","  inflating: Collection5/548.txt     \n","  inflating: Collection5/549.ann     \n","  inflating: Collection5/549.txt     \n","  inflating: Collection5/550.ann     \n","  inflating: Collection5/550.txt     \n","  inflating: Collection5/551.ann     \n","  inflating: Collection5/551.txt     \n","  inflating: Collection5/552.ann     \n","  inflating: Collection5/552.txt     \n","  inflating: Collection5/553.ann     \n","  inflating: Collection5/553.txt     \n","  inflating: Collection5/554.ann     \n","  inflating: Collection5/554.txt     \n","  inflating: Collection5/555 (!).ann  \n","  inflating: Collection5/555 (!).txt  \n","  inflating: Collection5/556.ann     \n","  inflating: Collection5/556.txt     \n","  inflating: Collection5/557.ann     \n","  inflating: Collection5/557.txt     \n","  inflating: Collection5/558.ann     \n","  inflating: Collection5/558.txt     \n","  inflating: Collection5/559.ann     \n","  inflating: Collection5/559.txt     \n","  inflating: Collection5/560.ann     \n","  inflating: Collection5/560.txt     \n","  inflating: Collection5/561.ann     \n","  inflating: Collection5/561.txt     \n","  inflating: Collection5/562.ann     \n","  inflating: Collection5/562.txt     \n","  inflating: Collection5/563.ann     \n","  inflating: Collection5/563.txt     \n","  inflating: Collection5/564.ann     \n","  inflating: Collection5/564.txt     \n","  inflating: Collection5/565.ann     \n","  inflating: Collection5/565.txt     \n","  inflating: Collection5/567.ann     \n","  inflating: Collection5/567.txt     \n","  inflating: Collection5/568.ann     \n","  inflating: Collection5/568.txt     \n","  inflating: Collection5/569.ann     \n","  inflating: Collection5/569.txt     \n","  inflating: Collection5/570.ann     \n","  inflating: Collection5/570.txt     \n","  inflating: Collection5/571.ann     \n","  inflating: Collection5/571.txt     \n","  inflating: Collection5/572.ann     \n","  inflating: Collection5/572.txt     \n","  inflating: Collection5/574.ann     \n","  inflating: Collection5/574.txt     \n","  inflating: Collection5/575.ann     \n","  inflating: Collection5/575.txt     \n","  inflating: Collection5/576.ann     \n","  inflating: Collection5/576.txt     \n","  inflating: Collection5/577.ann     \n","  inflating: Collection5/577.txt     \n","  inflating: Collection5/578.ann     \n","  inflating: Collection5/578.txt     \n","  inflating: Collection5/579.ann     \n","  inflating: Collection5/579.txt     \n","  inflating: Collection5/581.ann     \n","  inflating: Collection5/581.txt     \n","  inflating: Collection5/582.ann     \n","  inflating: Collection5/582.txt     \n","  inflating: Collection5/583.ann     \n","  inflating: Collection5/583.txt     \n","  inflating: Collection5/584 (!).ann  \n","  inflating: Collection5/584 (!).txt  \n","  inflating: Collection5/585.ann     \n","  inflating: Collection5/585.txt     \n","  inflating: Collection5/586.ann     \n","  inflating: Collection5/586.txt     \n","  inflating: Collection5/587.ann     \n","  inflating: Collection5/587.txt     \n","  inflating: Collection5/588.ann     \n","  inflating: Collection5/588.txt     \n","  inflating: Collection5/589.ann     \n","  inflating: Collection5/589.txt     \n","  inflating: Collection5/590.ann     \n","  inflating: Collection5/590.txt     \n","  inflating: Collection5/591.ann     \n","  inflating: Collection5/591.txt     \n","  inflating: Collection5/592.ann     \n","  inflating: Collection5/592.txt     \n","  inflating: Collection5/593.ann     \n","  inflating: Collection5/593.txt     \n","  inflating: Collection5/594.ann     \n","  inflating: Collection5/594.txt     \n","  inflating: Collection5/595.ann     \n","  inflating: Collection5/595.txt     \n","  inflating: Collection5/596.ann     \n","  inflating: Collection5/596.txt     \n","  inflating: Collection5/597.ann     \n","  inflating: Collection5/597.txt     \n","  inflating: Collection5/598 (!).ann  \n","  inflating: Collection5/598 (!).txt  \n","  inflating: Collection5/599.ann     \n","  inflating: Collection5/599.txt     \n","  inflating: Collection5/600.ann     \n","  inflating: Collection5/600.txt     \n","  inflating: Collection5/601.ann     \n","  inflating: Collection5/601.txt     \n","  inflating: Collection5/602.ann     \n","  inflating: Collection5/602.txt     \n","  inflating: Collection5/610.ann     \n","  inflating: Collection5/610.txt     \n","  inflating: Collection5/611.ann     \n","  inflating: Collection5/611.txt     \n","  inflating: Collection5/612.ann     \n","  inflating: Collection5/612.txt     \n","  inflating: Collection5/613.ann     \n","  inflating: Collection5/613.txt     \n","  inflating: Collection5/614.ann     \n","  inflating: Collection5/614.txt     \n","  inflating: Collection5/615.ann     \n","  inflating: Collection5/615.txt     \n","  inflating: Collection5/616.ann     \n","  inflating: Collection5/616.txt     \n","  inflating: Collection5/617.ann     \n","  inflating: Collection5/617.txt     \n","  inflating: Collection5/618.ann     \n","  inflating: Collection5/618.txt     \n","  inflating: Collection5/619.ann     \n","  inflating: Collection5/619.txt     \n","  inflating: Collection5/620.ann     \n","  inflating: Collection5/620.txt     \n","  inflating: Collection5/621.ann     \n","  inflating: Collection5/621.txt     \n","  inflating: Collection5/622.ann     \n","  inflating: Collection5/622.txt     \n","  inflating: Collection5/623.ann     \n","  inflating: Collection5/623.txt     \n","  inflating: Collection5/624.ann     \n","  inflating: Collection5/624.txt     \n","  inflating: Collection5/625.ann     \n","  inflating: Collection5/625.txt     \n","  inflating: Collection5/626.ann     \n","  inflating: Collection5/626.txt     \n","  inflating: Collection5/627.ann     \n","  inflating: Collection5/627.txt     \n","  inflating: Collection5/628.ann     \n","  inflating: Collection5/628.txt     \n","  inflating: Collection5/629.ann     \n","  inflating: Collection5/629.txt     \n","  inflating: Collection5/630.ann     \n","  inflating: Collection5/630.txt     \n","  inflating: Collection5/631.ann     \n","  inflating: Collection5/631.txt     \n","  inflating: Collection5/632.ann     \n","  inflating: Collection5/632.txt     \n","  inflating: Collection5/633.ann     \n","  inflating: Collection5/633.txt     \n","  inflating: Collection5/abdulatipov.ann  \n","  inflating: Collection5/abdulatipov.txt  \n","  inflating: Collection5/artjakov.ann  \n","  inflating: Collection5/artjakov.txt  \n","  inflating: Collection5/Avtovaz.ann  \n","  inflating: Collection5/Avtovaz.txt  \n","  inflating: Collection5/blokhin.ann  \n","  inflating: Collection5/blokhin.txt  \n","  inflating: Collection5/chaves.ann  \n","  inflating: Collection5/chaves.txt  \n","  inflating: Collection5/chirkunov.ann  \n","  inflating: Collection5/chirkunov.txt  \n","  inflating: Collection5/kamchatka.ann  \n","  inflating: Collection5/kamchatka.txt  \n","  inflating: Collection5/klinton.ann  \n","  inflating: Collection5/klinton.txt  \n","  inflating: Collection5/kuleshov.ann  \n","  inflating: Collection5/kuleshov.txt  \n","  inflating: Collection5/last_01.ann  \n","  inflating: Collection5/last_01.txt  \n","  inflating: Collection5/last_02.ann  \n","  inflating: Collection5/last_02.txt  \n","  inflating: Collection5/last_03.ann  \n","  inflating: Collection5/last_03.txt  \n","  inflating: Collection5/last_04.ann  \n","  inflating: Collection5/last_04.txt  \n","  inflating: Collection5/last_05.ann  \n","  inflating: Collection5/last_05.txt  \n","  inflating: Collection5/last_06.ann  \n","  inflating: Collection5/last_06.txt  \n","  inflating: Collection5/last_07_new.ann  \n","  inflating: Collection5/last_07_new.txt  \n","  inflating: Collection5/last_08.ann  \n","  inflating: Collection5/last_08.txt  \n","  inflating: Collection5/last_09.ann  \n","  inflating: Collection5/last_09.txt  \n","  inflating: Collection5/last_10.ann  \n","  inflating: Collection5/last_10.txt  \n","  inflating: Collection5/last_11.ann  \n","  inflating: Collection5/last_11.txt  \n","  inflating: Collection5/last_12.ann  \n","  inflating: Collection5/last_12.txt  \n","  inflating: Collection5/last_13.ann  \n","  inflating: Collection5/last_13.txt  \n","  inflating: Collection5/last_14.ann  \n","  inflating: Collection5/last_14.txt  \n","  inflating: Collection5/last_15.ann  \n","  inflating: Collection5/last_15.txt  \n","  inflating: Collection5/last_16.ann  \n","  inflating: Collection5/last_16.txt  \n","  inflating: Collection5/last_17.ann  \n","  inflating: Collection5/last_17.txt  \n","  inflating: Collection5/last_18.ann  \n","  inflating: Collection5/last_18.txt  \n","  inflating: Collection5/last_19.ann  \n","  inflating: Collection5/last_19.txt  \n","  inflating: Collection5/last_20.ann  \n","  inflating: Collection5/last_20.txt  \n","  inflating: Collection5/last_21.ann  \n","  inflating: Collection5/last_21.txt  \n","  inflating: Collection5/last_22.ann  \n","  inflating: Collection5/last_22.txt  \n","  inflating: Collection5/last_23.ann  \n","  inflating: Collection5/last_23.txt  \n","  inflating: Collection5/last_24.ann  \n","  inflating: Collection5/last_24.txt  \n","  inflating: Collection5/last_25.ann  \n","  inflating: Collection5/last_25.txt  \n","  inflating: Collection5/last_26.ann  \n","  inflating: Collection5/last_26.txt  \n","  inflating: Collection5/last_27.ann  \n","  inflating: Collection5/last_27.txt  \n","  inflating: Collection5/last_28.ann  \n","  inflating: Collection5/last_28.txt  \n","  inflating: Collection5/last_29.ann  \n","  inflating: Collection5/last_29.txt  \n","  inflating: Collection5/last_30_new.ann  \n","  inflating: Collection5/last_30_new.txt  \n","  inflating: Collection5/last_31.ann  \n","  inflating: Collection5/last_31.txt  \n","  inflating: Collection5/last_32.ann  \n","  inflating: Collection5/last_32.txt  \n","  inflating: Collection5/last_33.ann  \n","  inflating: Collection5/last_33.txt  \n","  inflating: Collection5/last_34.ann  \n","  inflating: Collection5/last_34.txt  \n","  inflating: Collection5/last_35.ann  \n","  inflating: Collection5/last_35.txt  \n","  inflating: Collection5/last_36.ann  \n","  inflating: Collection5/last_36.txt  \n","  inflating: Collection5/last_37.ann  \n","  inflating: Collection5/last_37.txt  \n","  inflating: Collection5/last_38.ann  \n","  inflating: Collection5/last_38.txt  \n","  inflating: Collection5/last_39.ann  \n","  inflating: Collection5/last_39.txt  \n","  inflating: Collection5/last_40.ann  \n","  inflating: Collection5/last_40.txt  \n","  inflating: Collection5/last_41.ann  \n","  inflating: Collection5/last_41.txt  \n","  inflating: Collection5/last_42.ann  \n","  inflating: Collection5/last_42.txt  \n","  inflating: Collection5/last_43.ann  \n","  inflating: Collection5/last_43.txt  \n","  inflating: Collection5/last_44.ann  \n","  inflating: Collection5/last_44.txt  \n","  inflating: Collection5/last_45.ann  \n","  inflating: Collection5/last_45.txt  \n","  inflating: Collection5/last_46.ann  \n","  inflating: Collection5/last_46.txt  \n","  inflating: Collection5/last_47.ann  \n","  inflating: Collection5/last_47.txt  \n","  inflating: Collection5/last_48.ann  \n","  inflating: Collection5/last_48.txt  \n","  inflating: Collection5/last_49.ann  \n","  inflating: Collection5/last_49.txt  \n","  inflating: Collection5/last_50.ann  \n","  inflating: Collection5/last_50.txt  \n","  inflating: Collection5/last_51.ann  \n","  inflating: Collection5/last_51.txt  \n","  inflating: Collection5/last_52.ann  \n","  inflating: Collection5/last_52.txt  \n","  inflating: Collection5/last_53.ann  \n","  inflating: Collection5/last_53.txt  \n","  inflating: Collection5/last_54.ann  \n","  inflating: Collection5/last_54.txt  \n","  inflating: Collection5/last_55.ann  \n","  inflating: Collection5/last_55.txt  \n","  inflating: Collection5/last_56.ann  \n","  inflating: Collection5/last_56.txt  \n","  inflating: Collection5/last_57.ann  \n","  inflating: Collection5/last_57.txt  \n","  inflating: Collection5/last_58.ann  \n","  inflating: Collection5/last_58.txt  \n","  inflating: Collection5/last_59.ann  \n","  inflating: Collection5/last_59.txt  \n","  inflating: Collection5/last_60.ann  \n","  inflating: Collection5/last_60.txt  \n","  inflating: Collection5/last_61.ann  \n","  inflating: Collection5/last_61.txt  \n","  inflating: Collection5/last_62.ann  \n","  inflating: Collection5/last_62.txt  \n","  inflating: Collection5/last_63.ann  \n","  inflating: Collection5/last_63.txt  \n","  inflating: Collection5/last_64.ann  \n","  inflating: Collection5/last_64.txt  \n","  inflating: Collection5/last_65.ann  \n","  inflating: Collection5/last_65.txt  \n","  inflating: Collection5/last_66.ann  \n","  inflating: Collection5/last_66.txt  \n","  inflating: Collection5/last_67.ann  \n","  inflating: Collection5/last_67.txt  \n","  inflating: Collection5/last_68.ann  \n","  inflating: Collection5/last_68.txt  \n","  inflating: Collection5/last_69.ann  \n","  inflating: Collection5/last_69.txt  \n","  inflating: Collection5/last_70.ann  \n","  inflating: Collection5/last_70.txt  \n","  inflating: Collection5/last_71.ann  \n","  inflating: Collection5/last_71.txt  \n","  inflating: Collection5/last_72.ann  \n","  inflating: Collection5/last_72.txt  \n","  inflating: Collection5/last_73.ann  \n","  inflating: Collection5/last_73.txt  \n","  inflating: Collection5/last_74.ann  \n","  inflating: Collection5/last_74.txt  \n","  inflating: Collection5/last_75.ann  \n","  inflating: Collection5/last_75.txt  \n","  inflating: Collection5/lenoblast.ann  \n","  inflating: Collection5/lenoblast.txt  \n","  inflating: Collection5/maykl dzhekson.ann  \n","  inflating: Collection5/maykl dzhekson.txt  \n","  inflating: Collection5/mvd.ann     \n","  inflating: Collection5/mvd.txt     \n","  inflating: Collection5/mvd2.ann    \n","  inflating: Collection5/mvd2.txt    \n","  inflating: Collection5/rosobrnadzor.ann  \n","  inflating: Collection5/rosobrnadzor.txt  \n","  inflating: Collection5/ryadovoy chelah.ann  \n","  inflating: Collection5/ryadovoy chelah.txt  \n","  inflating: Collection5/semenenko.ann  \n","  inflating: Collection5/semenenko.txt  \n","  inflating: Collection5/shojgu1.ann  \n","  inflating: Collection5/shojgu1.txt  \n","  inflating: Collection5/shojgu3.ann  \n","  inflating: Collection5/shojgu3.txt  \n","  inflating: Collection5/shojgu4.ann  \n","  inflating: Collection5/shojgu4.txt  \n","  inflating: Collection5/shojgu6.ann  \n","  inflating: Collection5/shojgu6.txt  \n","  inflating: Collection5/si_tzjanpin.ann  \n","  inflating: Collection5/si_tzjanpin.txt  \n","  inflating: Collection5/sobjanin2.ann  \n","  inflating: Collection5/sobjanin2.txt  \n","  inflating: Collection5/turkmenija.ann  \n","  inflating: Collection5/turkmenija.txt  \n","  inflating: Collection5/uchitel.ann  \n","  inflating: Collection5/uchitel.txt  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JPxCa2LyH8gW","executionInfo":{"status":"ok","timestamp":1626548184795,"user_tz":-180,"elapsed":801,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}}},"source":["!rm collection5.zip"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7burz-bD3t4l","executionInfo":{"status":"ok","timestamp":1626548189269,"user_tz":-180,"elapsed":394,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"9e62142d-93aa-410b-9722-980ffd3bcd05"},"source":["!ls"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Collection5  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UEdS2pAS3fod","executionInfo":{"status":"ok","timestamp":1626548193558,"user_tz":-180,"elapsed":388,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}}},"source":["from corus import load_ne5\n","\n","dir = 'Collection5/'\n","records = load_ne5(dir)\n","rec = next(records)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TuzGqeSclcWw","executionInfo":{"status":"ok","timestamp":1626548196629,"user_tz":-180,"elapsed":6,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"3be505b7-26ab-4c1d-cd1a-4de431fa71ca"},"source":["rec.spans"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Ne5Span(\n","     index='T1',\n","     type='ORG',\n","     start=0,\n","     stop=12,\n","     text='Рособрнадзор'\n"," ), Ne5Span(\n","     index='T2',\n","     type='ORG',\n","     start=43,\n","     stop=55,\n","     text='Рособрнадзор'\n"," ), Ne5Span(\n","     index='T3',\n","     type='GEOPOLIT',\n","     start=104,\n","     stop=110,\n","     text='России'\n"," ), Ne5Span(\n","     index='T4',\n","     type='PER',\n","     start=111,\n","     stop=127,\n","     text='Дмитрий Медведев'\n"," ), Ne5Span(\n","     index='T5',\n","     type='ORG',\n","     start=151,\n","     stop=208,\n","     text='Федеральной службы по надзору в сфере образования и науки'\n"," ), Ne5Span(\n","     index='T6',\n","     type='PER',\n","     start=241,\n","     stop=256,\n","     text='Ивана Муравьева'\n"," ), Ne5Span(\n","     index='T7',\n","     type='PER',\n","     start=377,\n","     stop=391,\n","     text='Любовь Глебова'\n"," ), Ne5Span(\n","     index='T8',\n","     type='ORG',\n","     start=405,\n","     stop=419,\n","     text='Рособрнадзором'\n"," ), Ne5Span(\n","     index='T9',\n","     type='LOC',\n","     start=506,\n","     stop=524,\n","     text='Пензенскую область'\n"," ), Ne5Span(\n","     index='T10',\n","     type='ORG',\n","     start=527,\n","     stop=543,\n","     text='Совете Федерации'\n"," ), Ne5Span(\n","     index='T11',\n","     type='PER',\n","     start=560,\n","     stop=570,\n","     text='Л.Глебовой'\n"," ), Ne5Span(\n","     index='T12',\n","     type='MEDIA',\n","     start=573,\n","     stop=576,\n","     text='СМИ'\n"," ), Ne5Span(\n","     index='T13',\n","     type='PER',\n","     start=622,\n","     stop=632,\n","     text='И.Муравьев'\n"," ), Ne5Span(\n","     index='T14',\n","     type='PER',\n","     start=898,\n","     stop=911,\n","     text='Иван Муравьев'\n"," ), Ne5Span(\n","     index='T15',\n","     type='ORG',\n","     start=994,\n","     stop=1007,\n","     text='Рособрнадзора'\n"," ), Ne5Span(\n","     index='T16',\n","     type='ORG',\n","     start=1054,\n","     stop=1065,\n","     text='Минкомсвязи'\n"," ), Ne5Span(\n","     index='T17',\n","     type='PER',\n","     start=1066,\n","     stop=1084,\n","     text='Николая Никифорова'\n"," )]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q9c84CRFlf7s","executionInfo":{"status":"ok","timestamp":1626548206335,"user_tz":-180,"elapsed":3335,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"3f3705cc-4055-47db-de14-e49d509d2308"},"source":["!pip install razdel"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Collecting razdel\n","  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n","Installing collected packages: razdel\n","Successfully installed razdel-0.5.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3g9PoTmJllsI","executionInfo":{"status":"ok","timestamp":1626548210041,"user_tz":-180,"elapsed":783,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}}},"source":["from razdel import tokenize"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"7snD9_1Qll2T","executionInfo":{"status":"ok","timestamp":1626548218303,"user_tz":-180,"elapsed":4637,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}}},"source":["words_docs = []\n","for ix, rec in enumerate(records):\n","    words = []\n","    for token in tokenize(rec.text):\n","        is_person = False\n","        for person in rec.spans:\n","            if (token.start >= person.start) and (token.stop <= person.stop):\n","                is_person = True\n","                break\n","        words.append([token.text, 'PERSON' if is_person else 'NO_PERSON'])\n","    words_docs.extend(words)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"P5wgdUiOll9w","executionInfo":{"status":"ok","timestamp":1626548223661,"user_tz":-180,"elapsed":511,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}}},"source":["import pandas as pd"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"UJenxhewlmA-","executionInfo":{"status":"ok","timestamp":1626548226527,"user_tz":-180,"elapsed":4,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}}},"source":["df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yvUqbhDMl3ZZ","executionInfo":{"status":"ok","timestamp":1626548230578,"user_tz":-180,"elapsed":1060,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"5632fcb3-50c9-4b84-f274-523bddd8df6b"},"source":["df_words['tag'].value_counts()"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["NO_PERSON    219099\n","PERSON        46221\n","Name: tag, dtype: int64"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":670},"id":"SkFaghV9l3i0","executionInfo":{"status":"ok","timestamp":1626548236122,"user_tz":-180,"elapsed":13,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"672a3531-ac32-402b-c2a3-1f91d2c73121"},"source":["df_words.head(20)"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word</th>\n","      <th>tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Р</td>\n","      <td>PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>.</td>\n","      <td>PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Абрамович</td>\n","      <td>PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>переизбран</td>\n","      <td>NO_PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>на</td>\n","      <td>NO_PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>пост</td>\n","      <td>NO_PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>председателя</td>\n","      <td>NO_PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Думы</td>\n","      <td>PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Чукотки</td>\n","      <td>PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>.</td>\n","      <td>NO_PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Роман</td>\n","      <td>PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Абрамович</td>\n","      <td>PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>переизбран</td>\n","      <td>NO_PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>на</td>\n","      <td>NO_PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>пост</td>\n","      <td>NO_PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>председателя</td>\n","      <td>NO_PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Думы</td>\n","      <td>PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Чукотки</td>\n","      <td>PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>.</td>\n","      <td>NO_PERSON</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Как</td>\n","      <td>NO_PERSON</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            word        tag\n","0              Р     PERSON\n","1              .     PERSON\n","2      Абрамович     PERSON\n","3     переизбран  NO_PERSON\n","4             на  NO_PERSON\n","5           пост  NO_PERSON\n","6   председателя  NO_PERSON\n","7           Думы     PERSON\n","8        Чукотки     PERSON\n","9              .  NO_PERSON\n","10         Роман     PERSON\n","11     Абрамович     PERSON\n","12    переизбран  NO_PERSON\n","13            на  NO_PERSON\n","14          пост  NO_PERSON\n","15  председателя  NO_PERSON\n","16          Думы     PERSON\n","17       Чукотки     PERSON\n","18             .  NO_PERSON\n","19           Как  NO_PERSON"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"EW9bHconknfx"},"source":["### 1.взять нер из nltk"]},{"cell_type":"code","metadata":{"id":"uRuODJpkIqlv","executionInfo":{"status":"ok","timestamp":1626548131301,"user_tz":-180,"elapsed":1691,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}}},"source":["import nltk\n","from nltk import word_tokenize, pos_tag, ne_chunk"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L4Thrc-imlMK","executionInfo":{"status":"ok","timestamp":1626548260471,"user_tz":-180,"elapsed":4454,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"c431cebe-8612-47f7-e8f8-58b50830fb33"},"source":["nltk.download('maxent_ne_chunker')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('words')"],"execution_count":17,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WPQewgTPrgCC","executionInfo":{"status":"ok","timestamp":1626548266064,"user_tz":-180,"elapsed":411,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"9fb66965-3fc6-4e1c-f001-41e8e45e88db"},"source":["nltk.pos_tag(nltk.word_tokenize(rec.text))"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('Е.Примаков', 'JJ'),\n"," ('назначен', 'NNP'),\n"," ('главным', 'NNP'),\n"," ('по', 'NNP'),\n"," ('ГЛОНАССу', 'NNP'),\n"," ('Председателем', 'NNP'),\n"," ('обновленного', 'NNP'),\n"," ('совета', 'NNP'),\n"," ('директоров', 'NNP'),\n"," ('федерального', 'NNP'),\n"," ('сетевого', 'NNP'),\n"," ('оператора', 'NNP'),\n"," ('в', 'NNP'),\n"," ('сфере', 'NNP'),\n"," ('навигационной', 'NNP'),\n"," ('деятельности', 'NNP'),\n"," ('``', '``'),\n"," ('НИС', 'FW'),\n"," ('ГЛОНАСС', 'NN'),\n"," (\"''\", \"''\"),\n"," ('избран', 'JJ'),\n"," ('экс-глава', 'JJ'),\n"," ('Торгово-промышленной', 'JJ'),\n"," ('палаты', 'NN'),\n"," ('РФ', 'NNP'),\n"," ('Евгений', 'NNP'),\n"," ('Примаков', 'NNP'),\n"," ('.', '.'),\n"," ('Как', 'VB'),\n"," ('сообщили', 'JJ'),\n"," ('в', 'NNP'),\n"," ('пресс-службе', 'NN'),\n"," ('компании', 'NN'),\n"," (',', ','),\n"," ('на', 'NNP'),\n"," ('этом', 'NNP'),\n"," ('посту', 'NNP'),\n"," ('Е.Примаков', 'NNP'),\n"," ('сменит', 'NNP'),\n"," ('министра', 'NNP'),\n"," ('по', 'NNP'),\n"," ('делам', 'NNP'),\n"," ('гражданской', 'NNP'),\n"," ('обороны', 'NNP'),\n"," (',', ','),\n"," ('чрезвычайным', 'NNP'),\n"," ('ситуациям', 'NNP'),\n"," ('и', 'NNP'),\n"," ('ликвидации', 'NNP'),\n"," ('последствий', 'NNP'),\n"," ('стихийных', 'NNP'),\n"," ('бедствий', 'NNP'),\n"," ('(', '('),\n"," ('МЧС', 'NNP'),\n"," (')', ')'),\n"," ('РФ', 'VBP'),\n"," ('Сергея', 'JJ'),\n"," ('Шойгу', 'NNP'),\n"," ('.', '.'),\n"," ('Такое', 'VB'),\n"," ('решение', 'JJ'),\n"," ('было', 'NNP'),\n"," ('принято', 'NNP'),\n"," ('на', 'NNP'),\n"," ('общем', 'NNP'),\n"," ('годовом', 'NNP'),\n"," ('собрании', 'NNP'),\n"," ('акционеров', 'VBD'),\n"," ('30', 'CD'),\n"," ('июня', 'JJ'),\n"," ('2011г', 'CD'),\n"," ('.', '.'),\n"," ('Изменения', 'VB'),\n"," ('в', 'JJ'),\n"," ('составе', 'NNP'),\n"," ('совета', 'NNP'),\n"," ('директоров', 'NNP'),\n"," ('``', '``'),\n"," ('НИС', 'FW'),\n"," ('ГЛОНАСС', 'NN'),\n"," (\"''\", \"''\"),\n"," ('связаны', 'CC'),\n"," ('с', 'NNP'),\n"," ('поручением', 'NNP'),\n"," ('президента', 'NNP'),\n"," ('РФ', 'NNP'),\n"," ('Дмитрия', 'NNP'),\n"," ('Медведева', 'NNP'),\n"," ('о', 'NNP'),\n"," ('выводе', 'NNP'),\n"," ('госчиновников', 'NNP'),\n"," ('из', 'NNP'),\n"," ('советов', 'NNP'),\n"," ('директоров', 'NNP'),\n"," ('крупных', 'NNP'),\n"," ('компаний', 'NNP'),\n"," ('.', '.'),\n"," ('Акционеры', 'VB'),\n"," ('также', 'JJ'),\n"," ('избрали', 'NNP'),\n"," ('в', 'NNP'),\n"," ('новый', 'NNP'),\n"," ('совет', 'NNP'),\n"," ('трех', 'NNP'),\n"," ('независимых', 'NNP'),\n"," ('директоров', 'NNP'),\n"," ('-', ':'),\n"," ('президента', 'NN'),\n"," ('Ассоциации', 'JJ'),\n"," ('международных', 'NNP'),\n"," ('автомобильных', 'NNP'),\n"," ('перевозок', 'NNP'),\n"," ('Евгения', 'NNP'),\n"," ('Москвичева', 'NNP'),\n"," (',', ','),\n"," ('вице-президента', 'JJ'),\n"," ('Navteq', 'NNP'),\n"," ('Яниса', 'NNP'),\n"," ('Моисидиса', 'NNP'),\n"," ('и', 'NNP'),\n"," ('вице-президента', 'JJ'),\n"," ('по', 'NNP'),\n"," ('техническому', 'NNP'),\n"," ('развитию', 'NNP'),\n"," ('ОАО', 'NNP'),\n"," ('``', '``'),\n"," ('АВТОВАЗ', 'NN'),\n"," (\"''\", \"''\"),\n"," ('Евгения', 'NN'),\n"," ('Шмелева', 'NNP'),\n"," ('.', '.'),\n"," ('ОАО', 'VB'),\n"," ('``', '``'),\n"," ('Навигационно-информационные', 'JJ'),\n"," ('системы', 'NN'),\n"," (\"''\", \"''\"),\n"," ('было', 'CC'),\n"," ('образовано', 'NNP'),\n"," ('ФГУП', 'NNP'),\n"," ('``', '``'),\n"," ('Российский', 'JJ'),\n"," ('научно-исследовательский', 'JJ'),\n"," ('институт', 'NN'),\n"," ('космического', 'NN'),\n"," ('приборостроения', 'NN'),\n"," (\"''\", \"''\"),\n"," ('(', '('),\n"," ('49', 'CD'),\n"," ('%', 'NN'),\n"," (')', ')'),\n"," (',', ','),\n"," ('ОАО', 'VBZ'),\n"," ('``', '``'),\n"," ('Ситроникс', 'JJ'),\n"," (\"''\", \"''\"),\n"," ('(', '('),\n"," ('25,5', 'CD'),\n"," ('%', 'NN'),\n"," (')', ')'),\n"," ('и', 'VBZ'),\n"," ('``', '``'),\n"," ('РТИ', 'JJ'),\n"," ('Системы', 'NN'),\n"," (\"''\", \"''\"),\n"," ('(', '('),\n"," ('25,5', 'CD'),\n"," ('%', 'NN'),\n"," (')', ')'),\n"," ('с', 'NN'),\n"," ('целью', 'NNP'),\n"," ('коммерциализации', 'NNP'),\n"," ('разработок', 'NNP'),\n"," ('в', 'NNP'),\n"," ('рамках', 'NNP'),\n"," ('федеральной', 'NNP'),\n"," ('целевой', 'NNP'),\n"," ('программы', 'NNP'),\n"," ('``', '``'),\n"," ('Глобальная', 'NNP'),\n"," ('навигационная', 'NNP'),\n"," ('система', 'NNP'),\n"," (\"''\", \"''\"),\n"," ('.', '.')]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"skYaNCiC5xM4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626548302693,"user_tz":-180,"elapsed":386,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"71f2b4b1-363a-474f-e365-9165c9b2b31a"},"source":["{(' '.join(c[0] for c in chunk), chunk.label() ) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(rec.text))) if hasattr(chunk, 'label') }"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{('Navteq Яниса Моисидиса', 'PERSON'),\n"," ('Ассоциации', 'ORGANIZATION'),\n"," ('ГЛОНАСС', 'ORGANIZATION'),\n"," ('Глобальная', 'PERSON'),\n"," ('Дмитрия Медведева', 'PERSON'),\n"," ('Евгений Примаков', 'PERSON'),\n"," ('Евгения Москвичева', 'PERSON'),\n"," ('Евгения Шмелева', 'PERSON'),\n"," ('МЧС', 'ORGANIZATION'),\n"," ('Сергея Шойгу', 'PERSON')}"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"2cHR38QejPT1"},"source":["#### Вывод по nltk\n","NER из nltk классифицирует с ошибками именованные сущности в тексте на русском языке."]},{"cell_type":"markdown","metadata":{"id":"EYWdX9Fiyhmc"},"source":["### 3.написать свой нер, попробовать разные подходы (с доп информацией и без), также с учётом соседей и без них"]},{"cell_type":"code","metadata":{"id":"hY0U_QCcyiRd","executionInfo":{"status":"ok","timestamp":1626548775704,"user_tz":-180,"elapsed":10247,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}}},"source":["records = load_ne5(dir)\n","words_docs = []\n","for ix, rec in enumerate(records):\n","    words = []\n","    for token in tokenize(rec.text):\n","       \n","        result = 'None'        \n","        \n","        for item in rec.spans:            \n","            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'PER'):\n","                result = 'PER'\n","                break\n","            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'ORG'):\n","                result = 'ORG'\n","                break\n","            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'MEDIA'):\n","                result = 'MEDIA'\n","                break\n","            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'LOC'):\n","                result = 'LOC'\n","                break\n","            if (token.start >= item.start) and (token.stop <= item.stop) and (item.type == 'GEOPOLIT'):\n","                result = 'GEOPOLIT'\n","                break\n","                \n","    \n","        words.append([token.text, result])\n","    words_docs.extend(words)"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"burewGEEyiV7","executionInfo":{"status":"ok","timestamp":1626548664350,"user_tz":-180,"elapsed":392,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}}},"source":["df_words = pd.DataFrame(words_docs, columns=['word', 'tag'])"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"w9KIifvByiYm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626548666291,"user_tz":-180,"elapsed":4,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"ccb9f6b9-6f97-4367-fd68-d143a53dbfab"},"source":["df_words['tag'].value_counts()"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["None        219214\n","PER          21200\n","ORG          13651\n","LOC           4568\n","GEOPOLIT      4356\n","MEDIA         2482\n","Name: tag, dtype: int64"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"fVU6FpK_yibM","colab":{"base_uri":"https://localhost:8080/","height":670},"executionInfo":{"status":"ok","timestamp":1626550131002,"user_tz":-180,"elapsed":13,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"ca0831bd-d5af-4f5a-8389-5064d677ba56"},"source":["df_words.head(20)"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word</th>\n","      <th>tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Рособрнадзор</td>\n","      <td>ORG</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>возглавил</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>30-летний</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>чиновник</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Рособрнадзор</td>\n","      <td>ORG</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>возглавил</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>30-летний</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>чиновник</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Премьер-министр</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>России</td>\n","      <td>GEOPOLIT</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Дмитрий</td>\n","      <td>PER</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Медведев</td>\n","      <td>PER</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>назначил</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>руководителем</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Федеральной</td>\n","      <td>ORG</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>службы</td>\n","      <td>ORG</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>по</td>\n","      <td>ORG</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>надзору</td>\n","      <td>ORG</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>в</td>\n","      <td>ORG</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>сфере</td>\n","      <td>ORG</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               word       tag\n","0      Рособрнадзор       ORG\n","1         возглавил      None\n","2         30-летний      None\n","3          чиновник      None\n","4      Рособрнадзор       ORG\n","5         возглавил      None\n","6         30-летний      None\n","7          чиновник      None\n","8   Премьер-министр      None\n","9            России  GEOPOLIT\n","10          Дмитрий       PER\n","11         Медведев       PER\n","12         назначил      None\n","13    руководителем      None\n","14      Федеральной       ORG\n","15           службы       ORG\n","16               по       ORG\n","17          надзору       ORG\n","18                в       ORG\n","19            сфере       ORG"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"KgKF5LK6yidx","executionInfo":{"status":"ok","timestamp":1626548858868,"user_tz":-180,"elapsed":1808,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}}},"source":["import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout, Input\n","from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"RYQkw6F-zLFh","executionInfo":{"status":"ok","timestamp":1626548862343,"user_tz":-180,"elapsed":525,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}}},"source":["from sklearn import model_selection, preprocessing, linear_model\n","\n","train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df_words['word'], df_words['tag'])\n","\n","# labelEncode целевую переменную\n","encoder = preprocessing.LabelEncoder()\n","train_y = encoder.fit_transform(train_y)\n","valid_y = encoder.fit_transform(valid_y)"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"2b9NoW_0zLRZ","executionInfo":{"status":"ok","timestamp":1626548871629,"user_tz":-180,"elapsed":5715,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}}},"source":["train_data = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n","valid_data = tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n","\n","train_data = train_data.batch(16)\n","valid_data = valid_data.batch(16)"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"8QHVkjiYyihl","executionInfo":{"status":"ok","timestamp":1626548874240,"user_tz":-180,"elapsed":403,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}}},"source":["AUTOTUNE = tf.data.AUTOTUNE\n","\n","train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n","valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"NUaRKD4mzWm4","executionInfo":{"status":"ok","timestamp":1626548878519,"user_tz":-180,"elapsed":501,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}}},"source":["def custom_standardization(input_data):\n","        return input_data\n","\n","def data_prep(train_data, seq_len=1, vocab_size = 30000):    \n","    \n","    vocab_size = 30000\n","    #seq_len = 1\n","\n","    vectorize_layer = TextVectorization(\n","        standardize=custom_standardization,\n","        max_tokens=vocab_size,\n","        output_mode='int',\n","        output_sequence_length=seq_len)\n","\n","\n","    # Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n","    text_data = train_data.map(lambda x, y: x)\n","    vectorize_layer.adapt(text_data)\n","    return vectorize_layer"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"MXAdLTuNzZTw","executionInfo":{"status":"ok","timestamp":1626548881937,"user_tz":-180,"elapsed":4,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}}},"source":["embedding_dim = 64\n","\n","class modelNER(tf.keras.Model):\n","    def __init__(self):\n","        super(modelNER, self).__init__()\n","        self.emb = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gPool = tf.keras.layers.GlobalMaxPooling1D()\n","        self.fc1 = tf.keras.layers.Dense(300, activation='relu')\n","        self.fc2 = tf.keras.layers.Dense(50, activation='relu')\n","        self.fc3 = tf.keras.layers.Dense(len(df_words['tag'].value_counts()), activation='softmax')\n","\n","    def call(self, x):\n","        x = vectorize_layer(x)\n","        x = self.emb(x)\n","        pool_x = self.gPool(x)\n","        \n","        fc_x = self.fc1(pool_x)\n","        fc_x = self.fc2(fc_x)\n","        \n","        concat_x = tf.concat([pool_x, fc_x], axis=1)\n","        return self.fc3(concat_x)"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"xO5KrZXNzcHa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626550129048,"user_tz":-180,"elapsed":1244121,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"265457d3-85e6-4f6f-fc5d-fbc117ab0897"},"source":["vocab_size = 30000\n","vectorize_layer = data_prep(train_data, seq_len = 1, vocab_size = vocab_size)\n","\n","\n","mmodel = modelNER()\n","mmodel.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","              metrics=['accuracy'])\n","mmodel.fit(train_data, validation_data=valid_data, epochs=5)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","12444/12444 [==============================] - 223s 18ms/step - loss: 0.2822 - accuracy: 0.9181 - val_loss: 0.2037 - val_accuracy: 0.9394\n","Epoch 2/5\n","12444/12444 [==============================] - 221s 18ms/step - loss: 0.1228 - accuracy: 0.9633 - val_loss: 0.2126 - val_accuracy: 0.9417\n","Epoch 3/5\n","12444/12444 [==============================] - 219s 18ms/step - loss: 0.1085 - accuracy: 0.9659 - val_loss: 0.2109 - val_accuracy: 0.9416\n","Epoch 4/5\n","12444/12444 [==============================] - 226s 18ms/step - loss: 0.1033 - accuracy: 0.9668 - val_loss: 0.2246 - val_accuracy: 0.9415\n","Epoch 5/5\n","12444/12444 [==============================] - 224s 18ms/step - loss: 0.1003 - accuracy: 0.9674 - val_loss: 0.2413 - val_accuracy: 0.9412\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fbea022a450>"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WX576iKG3sCk","executionInfo":{"status":"ok","timestamp":1626551684146,"user_tz":-180,"elapsed":1253013,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"e1849fa6-6643-48fb-ec6b-e2b956b22863"},"source":["vocab_size = 30000\n","vectorize_layer = data_prep(train_data, seq_len = 3, vocab_size = vocab_size)\n","\n","\n","mmodel = modelNER()\n","mmodel.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","              metrics=['accuracy'])\n","mmodel.fit(train_data, validation_data=valid_data, epochs=5)"],"execution_count":38,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","12444/12444 [==============================] - 229s 18ms/step - loss: 0.2948 - accuracy: 0.9142 - val_loss: 0.2059 - val_accuracy: 0.9384\n","Epoch 2/5\n","12444/12444 [==============================] - 230s 18ms/step - loss: 0.1272 - accuracy: 0.9621 - val_loss: 0.2134 - val_accuracy: 0.9416\n","Epoch 3/5\n","12444/12444 [==============================] - 229s 18ms/step - loss: 0.1105 - accuracy: 0.9656 - val_loss: 0.2104 - val_accuracy: 0.9423\n","Epoch 4/5\n","12444/12444 [==============================] - 229s 18ms/step - loss: 0.1046 - accuracy: 0.9666 - val_loss: 0.2268 - val_accuracy: 0.9421\n","Epoch 5/5\n","12444/12444 [==============================] - 227s 18ms/step - loss: 0.1014 - accuracy: 0.9671 - val_loss: 0.2215 - val_accuracy: 0.9408\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fbea012ff90>"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aKLUINiq3sQj","executionInfo":{"status":"ok","timestamp":1626553311717,"user_tz":-180,"elapsed":1278896,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"08e88cf7-d96f-492d-862a-d85e97e3a120"},"source":["vocab_size = 30000\n","vectorize_layer = data_prep(train_data, seq_len = 5, vocab_size = vocab_size)\n","\n","\n","mmodel = modelNER()\n","mmodel.compile(optimizer='adam',\n","              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n","              metrics=['accuracy'])\n","mmodel.fit(train_data, validation_data=valid_data, epochs=5)"],"execution_count":40,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","12444/12444 [==============================] - 224s 18ms/step - loss: 0.2962 - accuracy: 0.9133 - val_loss: 0.2073 - val_accuracy: 0.9380\n","Epoch 2/5\n","12444/12444 [==============================] - 225s 18ms/step - loss: 0.1259 - accuracy: 0.9624 - val_loss: 0.2624 - val_accuracy: 0.8940\n","Epoch 3/5\n","12444/12444 [==============================] - 225s 18ms/step - loss: 0.1088 - accuracy: 0.9656 - val_loss: 0.2780 - val_accuracy: 0.8933\n","Epoch 4/5\n","12444/12444 [==============================] - 224s 18ms/step - loss: 0.1037 - accuracy: 0.9665 - val_loss: 0.2877 - val_accuracy: 0.8928\n","Epoch 5/5\n","12444/12444 [==============================] - 225s 18ms/step - loss: 0.1010 - accuracy: 0.9671 - val_loss: 0.3056 - val_accuracy: 0.8931\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fbe8ec37ad0>"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"wI3Sy3T9Ixna"},"source":["#### Вывод по modelNER\n","В собственном нер подходы с учетом соседей и без мало различаются по результатам."]},{"cell_type":"markdown","metadata":{"id":"64rHv9lcnLd4"},"source":["### 2.проверить deeppavlov"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"2kd-emBao1u-","executionInfo":{"status":"ok","timestamp":1626537744854,"user_tz":-180,"elapsed":96790,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"9761bbe9-a1a7-42fe-f640-28fe14b46ba2"},"source":["# установка deeppavlov\n","\n","!pip uninstall -y tensorflow tensorflow-gpu\n","!pip install numpy scipy librosa unidecode inflect librosa transformers\n","!pip install deeppavlov"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Uninstalling tensorflow-2.5.0:\n","  Successfully uninstalled tensorflow-2.5.0\n","\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n","Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n","Collecting unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n","\u001b[K     |████████████████████████████████| 245kB 4.2MB/s \n","\u001b[?25hRequirement already satisfied: inflect in /usr/local/lib/python3.7/dist-packages (2.1.0)\n","Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/1a/41c644c963249fd7f3836d926afa1e3f1cc234a1c40d80c5f03ad8f6f1b2/transformers-4.8.2-py3-none-any.whl (2.5MB)\n","\u001b[K     |████████████████████████████████| 2.5MB 7.6MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.0)\n","Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.1)\n","Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.22.2.post1)\n","Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n","Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n","Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n","Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n","Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n","Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n","\u001b[K     |████████████████████████████████| 3.3MB 27.8MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n","\u001b[K     |████████████████████████████████| 901kB 42.6MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Collecting huggingface-hub==0.0.12\n","  Downloading https://files.pythonhosted.org/packages/2f/ee/97e253668fda9b17e968b3f97b2f8e53aa0127e8807d24a547687423fe0b/huggingface_hub-0.0.12-py3-none-any.whl\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (2.4.7)\n","Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.2.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.14.6)\n","Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.5.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.20)\n","Installing collected packages: unidecode, tokenizers, sacremoses, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2 unidecode-1.2.0\n","Collecting deeppavlov\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/87/e77ccc7de09f8c5c4a3d981ff6b1d3811d9978976a30bec9bdf50d667ebb/deeppavlov-0.15.0-py3-none-any.whl (907kB)\n","\u001b[K     |████████████████████████████████| 911kB 5.3MB/s \n","\u001b[?25hRequirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (3.0.12)\n","Collecting fastapi==0.47.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/a7/4804d7abf8a1544d079d50650af872387154ebdac5bd07d54b2e60e2b334/fastapi-0.47.1-py3-none-any.whl (43kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n","\u001b[?25hCollecting prometheus-client==0.7.1\n","  Downloading https://files.pythonhosted.org/packages/b3/23/41a5a24b502d35a4ad50a5bb7202a5e1d9a0364d0c12f56db3dbf7aca76d/prometheus_client-0.7.1.tar.gz\n","Collecting ruamel.yaml==0.15.100\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/fc/12de89822adaa3a60b8cb0139bae75918278999d08e6dff158623abd7cba/ruamel.yaml-0.15.100-cp37-cp37m-manylinux1_x86_64.whl (654kB)\n","\u001b[K     |████████████████████████████████| 655kB 28.9MB/s \n","\u001b[?25hCollecting pydantic==1.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/56/1f652c3f658d2a9fd495d2e988a2da57eabdb6c4b8f4563c2ccbe6a2a8c5/pydantic-1.3-cp37-cp37m-manylinux2010_x86_64.whl (7.3MB)\n","\u001b[K     |████████████████████████████████| 7.3MB 15.5MB/s \n","\u001b[?25hCollecting pyopenssl==19.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/de/f8342b68fa9e981d348039954657bdf681b2ab93de27443be51865ffa310/pyOpenSSL-19.1.0-py2.py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 8.0MB/s \n","\u001b[?25hCollecting pymorphy2==0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n","\u001b[?25hRequirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (1.4.1)\n","Collecting pandas==0.25.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/e0/a1b39cdcb2c391f087a1538bc8a6d62a82d0439693192aef541d7b123769/pandas-0.25.3-cp37-cp37m-manylinux1_x86_64.whl (10.4MB)\n","\u001b[K     |████████████████████████████████| 10.4MB 20.3MB/s \n","\u001b[?25hCollecting pytelegrambotapi==3.6.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/ab/99c606f69fcda57e35788b913dd34c9d9acb48dd26349141b3855dcf6351/pyTelegramBotAPI-3.6.7.tar.gz (65kB)\n","\u001b[K     |████████████████████████████████| 71kB 8.9MB/s \n","\u001b[?25hCollecting h5py==2.10.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 35.6MB/s \n","\u001b[?25hCollecting numpy==1.18.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/53/127cb49435bcf5d841baf8eafa030931c62a9eac577a641f8c2293d23371/numpy-1.18.0-cp37-cp37m-manylinux1_x86_64.whl (20.1MB)\n","\u001b[K     |████████████████████████████████| 20.1MB 1.3MB/s \n","\u001b[?25hCollecting uvicorn==0.11.7\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/5f/2bc87272f189662e129ddcd4807ad3ef83128b4df3a3482335f5f9790f24/uvicorn-0.11.7-py3-none-any.whl (43kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n","\u001b[?25hCollecting sacremoses==0.0.35\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n","\u001b[K     |████████████████████████████████| 860kB 36.3MB/s \n","\u001b[?25hCollecting rusenttokenize==0.0.5\n","  Downloading https://files.pythonhosted.org/packages/25/4c/a2f00be5def774a3df2e5387145f1cb54e324607ec4a7e23f573645946e7/rusenttokenize-0.0.5-py3-none-any.whl\n","Collecting requests==2.22.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n","\u001b[K     |████████████████████████████████| 61kB 6.8MB/s \n","\u001b[?25hCollecting scikit-learn==0.21.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/a4/a48bd4b0d15395362b561df7e7247de87291105eb736a3b2aaffebf437b9/scikit_learn-0.21.2-cp37-cp37m-manylinux1_x86_64.whl (6.7MB)\n","\u001b[K     |████████████████████████████████| 6.7MB 36.0MB/s \n","\u001b[?25hRequirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (7.1.2)\n","Requirement already satisfied: tqdm==4.41.1 in /usr/local/lib/python3.7/dist-packages (from deeppavlov) (4.41.1)\n","Collecting overrides==2.7.0\n","  Downloading https://files.pythonhosted.org/packages/ac/98/2430afd204c48ac0a529d439d7e22df8fa603c668d03456b5947cb59ec36/overrides-2.7.0.tar.gz\n","Collecting aio-pika==6.4.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/07/196a4115cbef31fa0c3dabdea146f02dffe5e49998341d20dbe2278953bc/aio_pika-6.4.1-py3-none-any.whl (40kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.8MB/s \n","\u001b[?25hCollecting nltk==3.4.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/1d/d925cfb4f324ede997f6d47bea4d9babba51b49e87a767c170b77005889d/nltk-3.4.5.zip (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 38.6MB/s \n","\u001b[?25hCollecting uvloop==0.14.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/7a/54a80c03b555af21680a2f3692947b43a0d576d90c4c18cace0fee1ccc0e/uvloop-0.14.0-cp37-cp37m-manylinux2010_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 19.3MB/s \n","\u001b[?25hCollecting Cython==0.29.14\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/58/2deb24de3c10cc4c0f09639b46f4f4b50059f0fdc785128a57dd9fdce026/Cython-0.29.14-cp37-cp37m-manylinux1_x86_64.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 29.3MB/s \n","\u001b[?25hCollecting pymorphy2-dicts-ru\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n","\u001b[K     |████████████████████████████████| 8.2MB 30.5MB/s \n","\u001b[?25hCollecting pytz==2019.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/73/fe30c2daaaa0713420d0382b16fbb761409f532c56bdcc514bf7b6262bb6/pytz-2019.1-py2.py3-none-any.whl (510kB)\n","\u001b[K     |████████████████████████████████| 512kB 36.1MB/s \n","\u001b[?25hCollecting starlette<=0.12.9,>=0.12.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/95/2220fe5bf287e693a6430d8ee36c681b0157035b7249ec08f8fb36319d16/starlette-0.12.9.tar.gz (46kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.7MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from pyopenssl==19.1.0->deeppavlov) (1.15.0)\n","Collecting cryptography>=2.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b2/26/7af637e6a7e87258b963f1731c5982fb31cd507f0d90d91836e446955d02/cryptography-3.4.7-cp36-abi3-manylinux2014_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 35.8MB/s \n","\u001b[?25hCollecting pymorphy2-dicts<3.0,>=2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n","\u001b[K     |████████████████████████████████| 7.1MB 23.2MB/s \n","\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2==0.8->deeppavlov) (0.6.2)\n","Collecting dawg-python>=0.7\n","  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from pandas==0.25.3->deeppavlov) (2.8.1)\n","Collecting httptools==0.1.*; sys_platform != \"win32\" and sys_platform != \"cygwin\" and platform_python_implementation != \"PyPy\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/2e/485131e3aa113929b425f83854fafc190aa7df716cbeb258c875752f0c6e/httptools-0.1.2-cp37-cp37m-manylinux1_x86_64.whl (219kB)\n","\u001b[K     |████████████████████████████████| 225kB 49.0MB/s \n","\u001b[?25hCollecting websockets==8.*\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/0b/3ebc752392a368af14dd24ee041683416ac6d2463eead94b311b11e41c82/websockets-8.1-cp37-cp37m-manylinux2010_x86_64.whl (79kB)\n","\u001b[K     |████████████████████████████████| 81kB 9.7MB/s \n","\u001b[?25hCollecting h11<0.10,>=0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.9MB/s \n","\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses==0.0.35->deeppavlov) (1.0.1)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (3.0.4)\n","Collecting idna<2.9,>=2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.22.0->deeppavlov) (1.24.3)\n","Collecting aiormq<4,>=3.2.0\n","  Downloading https://files.pythonhosted.org/packages/0b/c4/dc5b9d50c15af2ee187974a5a0c3f20c06cce6559eea4c065d372e846b6a/aiormq-3.3.1-py3-none-any.whl\n","Collecting yarl\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n","\u001b[K     |████████████████████████████████| 296kB 40.4MB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (1.14.6)\n","Collecting pamqp==2.3.0\n","  Downloading https://files.pythonhosted.org/packages/eb/56/afa06143361e640c9159d828dadc95fc9195c52c95b4a97d136617b0166d/pamqp-2.3.0-py2.py3-none-any.whl\n","Requirement already satisfied: typing-extensions>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from yarl->aio-pika==6.4.1->deeppavlov) (3.7.4.3)\n","Collecting multidict>=4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n","\u001b[K     |████████████████████████████████| 143kB 40.5MB/s \n","\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.8->pyopenssl==19.1.0->deeppavlov) (2.20)\n","Building wheels for collected packages: prometheus-client, pytelegrambotapi, sacremoses, overrides, nltk, starlette\n","  Building wheel for prometheus-client (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for prometheus-client: filename=prometheus_client-0.7.1-cp37-none-any.whl size=41404 sha256=4cf69cb93c06edf7b318d9e7ba7e4f612a27894193db2540217ebcd2d79fe50a\n","  Stored in directory: /root/.cache/pip/wheels/1c/54/34/fd47cd9b308826cc4292b54449c1899a30251ef3b506bc91ea\n","  Building wheel for pytelegrambotapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytelegrambotapi: filename=pyTelegramBotAPI-3.6.7-cp37-none-any.whl size=47177 sha256=05f5da4769f51febe11aebcc4a707214574eb6a60af75d841ff32e95926e1b29\n","  Stored in directory: /root/.cache/pip/wheels/23/40/18/8a34153f95ef0dc19e3954898e5a5079244b76a8afdd7d0ec5\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp37-none-any.whl size=883990 sha256=67ada28e0db09201a013b734ab4c83d2686423d9adcc8530a5c41abc432a31ff\n","  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for overrides: filename=overrides-2.7.0-cp37-none-any.whl size=5606 sha256=089a7687698d762b58af7d2a2f02aa0530410cd78b14c3d016b29baa0b161252\n","  Stored in directory: /root/.cache/pip/wheels/8c/7c/ef/80508418b67d87371c5b3de49e03eb22ee7c1d19affb5099f8\n","  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nltk: filename=nltk-3.4.5-cp37-none-any.whl size=1449923 sha256=e83ac4d09d9ce4e8c52d99d520cde914ef49ba2bc70971d218edd66aceb6c5ac\n","  Stored in directory: /root/.cache/pip/wheels/96/86/f6/68ab24c23f207c0077381a5e3904b2815136b879538a24b483\n","  Building wheel for starlette (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for starlette: filename=starlette-0.12.9-cp37-none-any.whl size=57254 sha256=1677380168440eef591a193dacc2960bc6b34d79d8cd09961b726a01b27c707f\n","  Stored in directory: /root/.cache/pip/wheels/1c/51/5b/3828d52e185cafad941c4291b6f70894d0794be28c70addae5\n","Successfully built prometheus-client pytelegrambotapi sacremoses overrides nltk starlette\n","\u001b[31mERROR: kapre 0.3.5 requires tensorflow>=2.0.0, which is not installed.\u001b[0m\n","\u001b[31mERROR: xarray 0.18.2 has requirement pandas>=1.0, but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.18.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement pandas~=1.1.0; python_version >= \"3.0\", but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.22.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: fbprophet 0.7.1 has requirement pandas>=1.0.4, but you'll have pandas 0.25.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: starlette, pydantic, fastapi, prometheus-client, ruamel.yaml, cryptography, pyopenssl, pymorphy2-dicts, dawg-python, pymorphy2, pytz, numpy, pandas, idna, requests, pytelegrambotapi, h5py, httptools, uvloop, websockets, h11, uvicorn, sacremoses, rusenttokenize, scikit-learn, overrides, multidict, yarl, pamqp, aiormq, aio-pika, nltk, Cython, pymorphy2-dicts-ru, deeppavlov\n","  Found existing installation: prometheus-client 0.11.0\n","    Uninstalling prometheus-client-0.11.0:\n","      Successfully uninstalled prometheus-client-0.11.0\n","  Found existing installation: pytz 2018.9\n","    Uninstalling pytz-2018.9:\n","      Successfully uninstalled pytz-2018.9\n","  Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Found existing installation: pandas 1.1.5\n","    Uninstalling pandas-1.1.5:\n","      Successfully uninstalled pandas-1.1.5\n","  Found existing installation: idna 2.10\n","    Uninstalling idna-2.10:\n","      Successfully uninstalled idna-2.10\n","  Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","  Found existing installation: sacremoses 0.0.45\n","    Uninstalling sacremoses-0.0.45:\n","      Successfully uninstalled sacremoses-0.0.45\n","  Found existing installation: scikit-learn 0.22.2.post1\n","    Uninstalling scikit-learn-0.22.2.post1:\n","      Successfully uninstalled scikit-learn-0.22.2.post1\n","  Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","  Found existing installation: Cython 0.29.23\n","    Uninstalling Cython-0.29.23:\n","      Successfully uninstalled Cython-0.29.23\n","Successfully installed Cython-0.29.14 aio-pika-6.4.1 aiormq-3.3.1 cryptography-3.4.7 dawg-python-0.7.2 deeppavlov-0.15.0 fastapi-0.47.1 h11-0.9.0 h5py-2.10.0 httptools-0.1.2 idna-2.8 multidict-5.1.0 nltk-3.4.5 numpy-1.18.0 overrides-2.7.0 pamqp-2.3.0 pandas-0.25.3 prometheus-client-0.7.1 pydantic-1.3 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymorphy2-dicts-ru-2.4.417127.4579844 pyopenssl-19.1.0 pytelegrambotapi-3.6.7 pytz-2019.1 requests-2.22.0 ruamel.yaml-0.15.100 rusenttokenize-0.0.5 sacremoses-0.0.35 scikit-learn-0.21.2 starlette-0.12.9 uvicorn-0.11.7 uvloop-0.14.0 websockets-8.1 yarl-1.6.3\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["nltk","numpy","pandas","pytz","sklearn"]}}},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"KY96lqBzsZJ_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1626537835521,"user_tz":-180,"elapsed":48576,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"5eba7a6b-ae4a-4884-ce0a-b4dc89e01aaf"},"source":["!python -m deeppavlov install squad_bert\n","!python -m deeppavlov install ner_ontonotes"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-07-17 16:03:06.858 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'squad_bert' as '/usr/local/lib/python3.7/dist-packages/deeppavlov/configs/squad/squad_bert.json'\n","Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n","  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-ned3x7qs\n","  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-ned3x7qs\n","Building wheels for collected packages: bert-dp\n","  Building wheel for bert-dp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bert-dp: filename=bert_dp-1.0-cp37-none-any.whl size=23593 sha256=d45f3ee7c7f29ff670f8b4f51d440f5a104fb58b7b0884fa106be2ce89aadb58\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-m088owbf/wheels/1e/41/94/886107eaf932532594886fd8bfc9cb9d4db632e94add49d326\n","Successfully built bert-dp\n","Installing collected packages: bert-dp\n","Successfully installed bert-dp-1.0\n","Collecting tensorflow==1.15.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/81/84fb7a323f9723f81edfc796d89e89aa95a9446ed7353c144195b3a3a3ba/tensorflow-1.15.2-cp37-cp37m-manylinux2010_x86_64.whl (110.5MB)\n","\u001b[K     |████████████████████████████████| 110.5MB 41kB/s \n","\u001b[?25hCollecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.12.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.17.3)\n","Collecting keras-applications>=1.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n","\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.18.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.12.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.34.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n","Collecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 35.8MB/s \n","\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 34.3MB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.36.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (57.2.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.3.4)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.6.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.5.0)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7557 sha256=427c45d2bc951ad95a0f33e6adda077e9cf71831bdce8c21fb2c1c6128c2ed67\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow-probability 0.13.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.18.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.2 which is incompatible.\u001b[0m\n","Installing collected packages: gast, keras-applications, tensorflow-estimator, tensorboard, tensorflow\n","  Found existing installation: gast 0.4.0\n","    Uninstalling gast-0.4.0:\n","      Successfully uninstalled gast-0.4.0\n","  Found existing installation: tensorflow-estimator 2.5.0\n","    Uninstalling tensorflow-estimator-2.5.0:\n","      Successfully uninstalled tensorflow-estimator-2.5.0\n","  Found existing installation: tensorboard 2.5.0\n","    Uninstalling tensorboard-2.5.0:\n","      Successfully uninstalled tensorboard-2.5.0\n","Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n","2021-07-17 16:03:42.121 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'ner_ontonotes' as '/usr/local/lib/python3.7/dist-packages/deeppavlov/configs/ner/ner_ontonotes.json'\n","Requirement already satisfied: tensorflow==1.15.2 in /usr/local/lib/python3.7/dist-packages (1.15.2)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.0.8)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.12.0)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.1)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.17.3)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.12.1)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.18.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.34.1)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.36.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (57.2.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.3.4)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.6.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.5.0)\n","Collecting gensim==3.8.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/93/c6011037f24e3106d13f3be55297bf84ece2bf15b278cc4776339dc52db5/gensim-3.8.1-cp37-cp37m-manylinux1_x86_64.whl (24.2MB)\n","\u001b[K     |████████████████████████████████| 24.2MB 129kB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.18.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.4.1)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (1.15.0)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gensim==3.8.1) (5.1.0)\n","Installing collected packages: gensim\n","  Found existing installation: gensim 3.6.0\n","    Uninstalling gensim-3.6.0:\n","      Successfully uninstalled gensim-3.6.0\n","Successfully installed gensim-3.8.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6KE7tpVWs1b7"},"source":["import deeppavlov\n","from deeppavlov import configs, build_model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iuhgDjBgcXGz","executionInfo":{"status":"ok","timestamp":1626542316229,"user_tz":-180,"elapsed":16280,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"8b172b32-2bf3-416d-ec2c-0ee8695cc9c1"},"source":["!python -m deeppavlov install ner_ontonotes_bert_mult"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-07-17 17:18:22.370 INFO in 'deeppavlov.core.common.file'['file'] at line 32: Interpreting 'ner_ontonotes_bert_mult' as '/usr/local/lib/python3.7/dist-packages/deeppavlov/configs/ner/ner_ontonotes_bert_mult.json'\n","Requirement already satisfied: tensorflow==1.15.2 in /usr/local/lib/python3.7/dist-packages (1.15.2)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.36.2)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.18.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.12.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.12.1)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.34.1)\n","Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.0.8)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.17.3)\n","Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n","Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.2)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.3.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (57.2.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (4.6.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.5.0)\n","Collecting git+https://github.com/deepmipt/bert.git@feat/multi_gpu\n","  Cloning https://github.com/deepmipt/bert.git (to revision feat/multi_gpu) to /tmp/pip-req-build-z5rg1dck\n","  Running command git clone -q https://github.com/deepmipt/bert.git /tmp/pip-req-build-z5rg1dck\n","Requirement already satisfied (use --upgrade to upgrade): bert-dp==1.0 from git+https://github.com/deepmipt/bert.git@feat/multi_gpu in /usr/local/lib/python3.7/dist-packages\n","Building wheels for collected packages: bert-dp\n","  Building wheel for bert-dp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bert-dp: filename=bert_dp-1.0-cp37-none-any.whl size=23593 sha256=2b8c009ac6541326f74ba5560210bdd150d44d84f0e88c7ee10271a0ef16a306\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-b7__et7q/wheels/1e/41/94/886107eaf932532594886fd8bfc9cb9d4db632e94add49d326\n","Successfully built bert-dp\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gBqbik9Wei-Z","executionInfo":{"status":"ok","timestamp":1626542934459,"user_tz":-180,"elapsed":457467,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"ebe1a5a4-ad2c-4fb5-e1a4-47cef32d129f"},"source":["ner_dp_model = build_model(configs.ner.ner_ontonotes_bert_mult, download=True)\n","rus_document = \"Нью-Йорк, США, 30 апреля 2020, 01:01 — REGNUM В администрации президента США Дональда Трампа планируют пройти все этапы создания вакцины от коронавируса в ускоренном темпе и выпустить 100 млн доз до конца 2020 года, передаёт агентство Bloomberg со ссылкой на осведомлённые источники\"\n","result_1 = ner_dp_model([rus_document])\n","\n","for i in range(len(result_1[0][0])):\n","     if result_1 [1][0][i] != 'O':\n","         print(result_1[0][0][i], result_1[1][0][i])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2021-07-17 17:21:17.966 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/deeppavlov_data/ner_ontonotes_bert_mult_v1.tar.gz to /root/.deeppavlov/ner_ontonotes_bert_mult_v1.tar.gz\n","100%|██████████| 1.32G/1.32G [05:33<00:00, 3.96MB/s]\n","2021-07-17 17:26:53.531 INFO in 'deeppavlov.core.data.utils'['utils'] at line 272: Extracting /root/.deeppavlov/ner_ontonotes_bert_mult_v1.tar.gz archive into /root/.deeppavlov/models\n","2021-07-17 17:27:33.156 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/multi_cased_L-12_H-768_A-12.zip download because of matching hashes\n","2021-07-17 17:27:33.600 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 115: [loading vocabulary from /root/.deeppavlov/models/ner_ontonotes_bert_mult/tag.dict]\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/contrib/crf/python/ops/crf.py:213: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/deeppavlov/core/models/tf_model.py:234: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["2021-07-17 17:28:08.853 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from /root/.deeppavlov/models/ner_ontonotes_bert_mult/model]\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Restoring parameters from /root/.deeppavlov/models/ner_ontonotes_bert_mult/model\n","Нью B-GPE\n","- I-GPE\n","Йорк I-GPE\n","США B-GPE\n","30 B-DATE\n","апреля I-DATE\n","2020 I-DATE\n","01 B-TIME\n",": I-TIME\n","01 I-TIME\n","США B-GPE\n","Дональда B-PERSON\n","Трампа I-PERSON\n","100 B-MONEY\n","млн I-MONEY\n","доз I-MONEY\n","конца B-DATE\n","2020 I-DATE\n","года I-DATE\n","Bloomberg B-ORG\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jhxZXddyh_U7"},"source":["#### Вывод по deeppavlov\n","В DeepPavlov мультиязычный NER версии ner_ontonotes_bert_mult отлично классифицирует слова русского текста по типу именованных сущностей, к которым они принадлежат (имена собственные, географические названия, даты, валюты и другие)."]},{"cell_type":"markdown","metadata":{"id":"n1b-ZjbymBzd"},"source":["### 4.Дополнительно: NER в STANZA"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8O3fQyPqmUBB","executionInfo":{"status":"ok","timestamp":1626544379260,"user_tz":-180,"elapsed":5199,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"4130969c-f25c-45da-f5b8-feac6a0a025b"},"source":["!pip install stanza"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting stanza\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/a0/baecd55c7f1108a4b2459f46dde7aa765e393c735662861fe94ba7bc4ba3/stanza-1.2.2-py3-none-any.whl (337kB)\n","\u001b[K     |████████████████████████████████| 337kB 4.2MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanza) (3.17.3)\n","Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from stanza) (1.9.0+cu102)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanza) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanza) (2.22.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanza) (1.18.0)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanza) (1.15.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanza) (1.24.3)\n","Installing collected packages: stanza\n","Successfully installed stanza-1.2.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":454,"referenced_widgets":["5bdd500a8e984cad8313e60c8f2966c8","8b05b7469e6142a9bb4d86c9d466fa33","4fd7e859e77e475785530187049bd15b","d6eed99c37dd4ef881af200f7cd1f5ae","8f63a092ecf343e8abc9fa91af75ceef","1efff42a44724a91a596b251d954e856","3a52e7036e08428b9cff66a3d18b285f","2c274ef21c4c443b820d1af0765d99e3","94a8435635dd4158b066e9cb463e0950","47b20d4afa3b4118a3d217bc2a0a9b40","0f1e9d44dc914391b56e6a61f4378c2a","06f01950e8504beb84d9da11a956bb5e","321a7902e6c8480b875cc368128034b4","475f5dcb4e234f209a2d6be444022590","a25e94f82f50473d8bb6c43a626fa16f","f380c5ef0529432c9d5f67d1c7d106f6"]},"id":"vjntkzvamBC8","executionInfo":{"status":"ok","timestamp":1626544527469,"user_tz":-180,"elapsed":122959,"user":{"displayName":"Olga Gurr","photoUrl":"","userId":"01511282637848373828"}},"outputId":"c54729c7-644a-4473-9080-d3a2f57acf2c"},"source":["import stanza\n","stanza.download('ru')\n","def stanza_nlp_ru(text):\n","  nlp = stanza.Pipeline(lang='ru', processors='tokenize,ner')\n","  doc = nlp(text)\n","  print(*[f'entity: {ent.text}\\ttype: {ent.type}' for sent in doc.sentences for ent in sent.ents], sep='\\n')\n","stanza_nlp_ru(rus_document)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5bdd500a8e984cad8313e60c8f2966c8","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading https://raw.githubusercontent.com/stanfordnlp…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["2021-07-17 17:53:27 INFO: Downloading default packages for language: ru (Russian)...\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94a8435635dd4158b066e9cb463e0950","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading http://nlp.stanford.edu/software/stanza/1.2.2…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["2021-07-17 17:55:23 INFO: Finished downloading models and saved to /root/stanza_resources.\n","2021-07-17 17:55:23 INFO: Loading these models for language: ru (Russian):\n","=========================\n","| Processor | Package   |\n","-------------------------\n","| tokenize  | syntagrus |\n","| ner       | wikiner   |\n","=========================\n","\n","2021-07-17 17:55:23 INFO: Use device: cpu\n","2021-07-17 17:55:23 INFO: Loading: tokenize\n","2021-07-17 17:55:23 INFO: Loading: ner\n","2021-07-17 17:55:25 INFO: Done loading processors!\n"],"name":"stderr"},{"output_type":"stream","text":["entity: Нью-Йорк\ttype: LOC\n","entity: США\ttype: LOC\n","entity: США\ttype: LOC\n","entity: Дональда Трампа\ttype: PER\n","entity: Bloomberg\ttype: ORG\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"s8K8541hoCXk"},"source":["#### Вывод по STANZA\n","Модель с NER в STANZA отработала быстрее, чем NER в DeepPavlov. Результат для текста на русском языке - отличный."]},{"cell_type":"code","metadata":{"id":"AL4KGLXgtx_x"},"source":[""],"execution_count":null,"outputs":[]}]}